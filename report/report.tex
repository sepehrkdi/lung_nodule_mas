\documentclass[12pt]{article}
\usepackage[a4paper, total={5.7in, 9in}]{geometry}
\usepackage{url}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{soul}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning, fit}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10},
  keywordstyle=\color{blue},
  commentstyle=\color{green!50!black},
  stringstyle=\color{red!70!black},
  showstringspaces=false
}

\begin{document}
\title{BDI Multi-Agent System for Lung Nodule Classification from Chest X-ray Reports}
\author{Sepehr Khodadadi Hosseinabadi\\
{\em 6660699@studenti.unige.it}\\
\url{https://github.com/sepehrkdi/lung_nodule_mas}}
\maketitle

% =============================================================================
% PROPOSAL PAGE (not counted)
% =============================================================================
\section{Details of the Proposal}
\label{sec:yourProposal}

\subsection{Full Specification of the Proposal}
\label{subsec:specification}

I will build a BDI multi-agent system in which all agents are BDI agents that interoperate by message passing, while only the CV agents use a pre-trained model and the NLP agents are implemented by me. The use case is lung nodule classification from chest X-ray radiology reports, paired with the corresponding X-ray images from the IU/Open-I chest X-ray collection (English reports). The dataset contains around 7,470 images and reports. I will use an NLP-based pre-filter to select 500 image--report pairs and automatically derive binary ground-truth labels (benign/malignant) from the report text for evaluation of the multi-agent consensus and NLP extraction outputs (entities + attributes + negation/uncertainty).



\subsection{The Kind of the Proposal}
\label{subsec:kind}

My proposal consists of a \textbf{creative project}.

\subsection{The Range of Points/Difficulty of the Proposal}
\label{subsec:points}

The degree of difficulty for this project is \textbf{hard}.

\pagebreak

% =============================================================================
% INTRODUCTION
% =============================================================================
\section{Introduction}
\label{sec:intro}

Lung cancer is the leading cause of cancer-related death worldwide, and early detection of pulmonary nodules is critical for improving patient outcomes. Radiology reports offer a rich source of structured and unstructured clinical information, yet extracting actionable evidence from free-text\footnote{free-text reports are reports that are not structured in a way that can be easily processed by a computer} reports remains a challenging NLP task. Simultaneously, medical imaging with deep learning has made progress in automated nodule detection, but individual models vary in their sensitivity and specificity.

This project addresses the problem of \emph{lung nodule classification} by combining three AI paradigms within a multi-agent architecture:

\begin{enumerate}
    \item \textbf{Natural Language Processing (NLP):} Extracting structured nodule attributes (size, location, texture, descriptors), detecting mentions\footnote{mentions are the specific instances of nodule attributes mentioned in the report}, and determining negation/uncertainty status from free-text radiology reports.
    \item \textbf{Computer Vision (CV):} Obtaining nodule suspicion scores from chest X-ray images using the \texttt{TorchXRayVision} library~\cite{cohen2022torchxrayvision}, leveraging models pretrained on massive chest X-ray datasets.
    \item \textbf{Symbolic Reasoning:} Combining the outputs of multiple NLP and CV agents using first-order logic rules encoded in Prolog, implementing weighted consensus, conflict detection, and binary classification (0=benign, 1=malignant) with supplementary clinical recommendations.
\end{enumerate}

\subsection{Clinical Motivation}
\label{subsec:clinical-motivation}

In standard clinical practice for oncology patients, including those with lung malignancies, care is coordinated through multidisciplinary team (MDT) frameworks in which specialists such as radiologists, pathologists, surgeons, and oncologists review imaging and clinical information to formulate diagnosis and management plans~\cite{pillay2016mdt}. Direct communication between radiologists and pathologists is uncommon in routine practice because they work independently and send reports to the treating physician. This architecture is reflected in my system design, where all agents report to the central Oncologist agent rather than communicating peer-to-peer.

\subsection{Dataset and Pre-processing}
\label{subsec:dataset}

The system operates on the IU/Open-I Indiana University Chest X-ray Collection~\cite{demsar2005openi}, which contains 7,470 chest X-ray images (totaling $\sim$10\,GB) paired with free-text radiology reports. Detailed instructions for downloading and setting up this dataset are provided in the project's \href{https://github.com/sepehrkdi/lung_nodule_mas?tab=readme-ov-file#dataset}{\texttt{README.md}} file. This subset size is consistent with prior clinical NLP validation studies. For instance, Chapman et al.\ evaluated the NegEx negation-detection algorithm against a set of 1000 sentences containing 1235 clinical findings and diseases extracted from discharge summaries~\cite{chapman2001negex}. To focus the evaluation on cases where the NLP agents can perform meaningful extraction, I implemented a multi-stage pre-filter pipeline that combines XML metadata analysis with a composite NLP-richness scoring function:

\begin{enumerate}
    \item \textbf{Ingestion and MeSH\footnote{Medical Subject Headings} Parsing:} The \href{https://github.com/sepehrkdi/lung_nodule_mas/blob/main/data/nlmcxr_loader.py}{\texttt{NLMCXR\_Loader}} scans all XML report files. In addition to extracting the standard report sections (\texttt{FINDINGS}, \texttt{IMPRESSION}, \texttt{INDICATION}, \texttt{COMPARISON}), the parser now extracts \texttt{<MeSH>} annotations from each XML file. These include \texttt{<major>} tags (expert-assigned indexing terms such as ``Opacity/lung/upper lobe/right'' or ``normal'') and \texttt{<automatic>} tags (machine-generated terms).
    \item \textbf{NLP Richness Scoring:} Each case is scored on six binary criteria (each worth 1 point, yielding a score in $[0, 6]$):
    \begin{enumerate}
        \item \emph{Text length:} \texttt{FINDINGS} contains $\geq 80$ characters. Empirical analysis (available in \href{https://github.com/sepehrkdi/lung_nodule_mas/blob/main/report/report_length_analysis.txt}{\texttt{report\_length\_analysis.txt}} ) of the dataset indicates that reports below this threshold (approximately the bottom 3\%) are typically ``normal'' templates lacking specific findings, whereas reports above this length contain extractable pathological entities.
        \item \emph{Non-normal MeSH:} at least one \texttt{<major>} tag that is not just ``normal'', leveraging the expert indexing provided with the collection~\cite{demsar2005openi}.
        \item \emph{Target entity present:} regex match finds a clinical entity (nodule, mass, opacity, consolidation, etc.).
        \item \emph{Entity not fully negated:} at least one entity is \emph{not} linguistically negated, ensuring the report contains positive evidence~\cite{chapman2001negex, peng2018negbio}.
        \item \emph{Both sections non-empty:} \texttt{FINDINGS} and \texttt{IMPRESSION} both contain text.
        \item \emph{Anatomical location specified:} a laterality + anatomy pattern is present (e.g., ``right upper lobe'', ``left lung base'').
    \end{enumerate}
    \item \textbf{Ground-Truth Derivation:} Each case is independently passed through the NLP-based ground-truth extractor (Section~\ref{subsec:groundtruth}), which utilizes \emph{only the \texttt{IMPRESSION} section} to assign a binary label: 1 (malignant) or 0 (benign). This separation ensures the ground truth reflects the radiologist's synthesized diagnosis. Cases returning $-1$ (Indeterminate) are excluded.
    \item \textbf{Subset Selection:} Cases with richness score $\geq 3$ are retained and ranked in descending order of score. The top 500 cases are selected to form the \emph{Evaluation Subset}. In my dataset, 2{,}898 of 3{,}955 cases score $\geq 3$, providing candidates; 420 cases achieve the maximum score of 6.
\end{enumerate}

Table~\ref{tab:richness-dist} shows the score distribution throughout the whole dataset.

\begin{table}[ht]
\centering
\begin{tabular}{ccccccc}
\toprule
\textbf{Score} & 0 & 1 & 2 & 3 & 4 & 5--6 \\
\midrule
\textbf{Cases} & 84 & 92 & 793 & 1{,}210 & 770 & 1{,}006 \\
\bottomrule
\end{tabular}
\caption{Distribution of NLP richness scores across all 3{,}955 NLMCXR cases. Cases with score $\geq 3$ (77\%) are eligible for the Evaluation Subset.}
\label{tab:richness-dist}
\end{table}

This richness-based filtering ensures that the selected cases contain sufficient textual content for the Pathologist agents' regex, NER, and negation-detection pipelines to operate on, rather than relying only on binary keyword presence.

\subsection{Ground Truth Definition}
\label{subsec:groundtruth}

For the entire dataset, a binary label (1=malignant, 0=benign) is derived from the \texttt{IMPRESSION} section of the original radiology reports. The \texttt{IMPRESSION} section represents the radiologist's final diagnosis and synthesis. A case is labeled ``malignant'' (1) if this section contains un-negated suspicious keywords (``suspicious'', ``malignant'', ``ill-defined'', ``spiculated'', etc), otherwise ``benign'' (0).

Using NLP-derived labels as ground truth is an established practice in large-scale medical imaging research where manual annotation at scale is infeasible. Two datasets adopt this approach: CheXpert~\cite{irvin2019chexpert} applies a rule-based NLP labeler (mention extraction $\rightarrow$ negation/uncertainty classification $\rightarrow$ label aggregation) to assign 14 observation labels from radiology reports, while MIMIC-CXR-JPG~\cite{johnson2019mimiccxr} reuses the same CheXpert labeler to produce labels for over 370{,}000 chest radiographs. Both projects demonstrate that designed rule-based NLP pipelines produce labels of sufficient quality for evaluating models at scale.

My ground-truth extraction pipeline follows the same architectural pattern as the CheXpert labeler:
\begin{enumerate}
    \item \emph{Mention extraction}: keyword and regex patterns identify clinical entities (nodule, mass, opacity, spiculated, etc.);
    \item \emph{Negation and uncertainty detection}: NegEx-style~\cite{chapman2001negex} rules classify whether entities are affirmed, negated, or uncertain (cf.\ NegBio~\cite{peng2018negbio});
    \item \emph{Label aggregation}: the final binary label is derived from the presence of un-negated suspicious keywords.
\end{enumerate}
The key difference is scope: while CheXpert labels 14 observations, my extractor targets a single binary decision (benign vs.\ malignant) optimized for lung nodule assessment.

Note that the NLP agents (Section~\ref{subsubsec:pathologists}) are restricted to analyzing only the \texttt{FINDINGS} section, which contains the raw clinical observations. This separation prevents ``circular confirmation'' where agents re-read the diagnosis they are supposed to infer. Note that neither CheXpert nor MIMIC-CXR enforce this separation since both apply their labeler to the full report text. My approach is therefore more conservative: the agents must \emph{infer} the diagnosis from observations, while the ground truth is derived from the radiologist's explicit synthesis.

\subsection{Research Questions}
\label{subsec:research-questions}

Given the clinical motivation and technical design described above, this project investigates the following research questions:

\begin{description}
    \item[RQ1] \textbf{Does multi-agent consensus outperform individual agents?}\\
    \emph{Hypothesis:} A weighted ensemble of six agents (3~CV + 3~NLP) achieves higher classification accuracy than the best-performing individual agent, because independent errors across modalities are partially cancelled by aggregation.\\
    \emph{Tested by:} Comparing ensemble accuracy against each single-agent accuracy (Section~\ref{subsec:empirical-validation}, Table~\ref{tab:agent-accuracies}).

    \item[RQ2] \textbf{Does dynamic per-case weighting improve classification over static or equal weighting?}\\
    \emph{Hypothesis:} Adjusting agent weights per-case based on information richness (image quality and report completeness) yields higher accuracy than fixed expert-assigned weights or uniform weights, because it down-weights agents operating on degraded inputs.\\
    \emph{Tested by:} Comparing dynamic, static, and equal weighting modes on the same evaluation subset (Section~\ref{subsec:empirical-validation}, Table~\ref{tab:validation-results}).

    \item[RQ3] \textbf{Does negation detection improve NLP-based classification accuracy?}\\
    \emph{Hypothesis:} NegEx-style negation and uncertainty detection reduces false positives caused by negated findings (e.g., ``no nodule''), improving precision and F1 score on reports that contain negated clinical entities.\\
    \emph{Tested by:} The NegEx ablation within the evaluation framework (Section~\ref{subsec:ablation-framework}, Table~\ref{tab:claims}, claim \texttt{negex\_contribution}).

    \item[RQ4] \textbf{Does the system outperform na\"ive baselines on imbalanced clinical data?}\\
    \emph{Hypothesis:} The full multi-agent pipeline achieves higher accuracy than the majority-class baseline (always predicting the most frequent class), demonstrating that the system's complexity is justified beyond a trivial prediction strategy.\\
    \emph{Tested by:} Comparing system accuracy against the majority-class prior (Section~\ref{subsec:empirical-validation}, Table~\ref{tab:validation-results}).
\end{description}

\subsection{Contributions}
\label{subsec:contributions}

The main contributions of this project are:

\begin{enumerate}
    \item A BDI multi-agent system with \textbf{7 agent instances} across 3 agent types, communicating via FIPA-ACL message passing.
    \item An evaluation methodology where NLP agents must infer a diagnosis from observational data (\texttt{FINDINGS}) while being evaluated against the radiologist's explicit diagnosis (\texttt{IMPRESSION}).
    \item A custom NLP pipeline for lung nodule classification implementing: report section splitting with section weighting, entity and attribute extraction, measurement normalization, and NegEx-style negation and uncertainty detection.
    \item A dependency-anchored frame building module with multi-pass traversal that handles long-distance dependencies in complex clinical constructions (e.g., ``\textit{A nodule, likely representing granuloma, measuring 5mm}''), utilizing clausal modifiers (\texttt{acl}, \texttt{relcl}, \texttt{appos}) and participial chain scanning to strictly associate attributes with the correct nodule entity.
    \item Graded uncertainty quantification that distinguishes aleatory uncertainty (inherent text ambiguity) from epistemic uncertainty (knowledge gaps), providing continuous scores rather than categorical labels and enabling downstream systems to reason about uncertainty sources.
    \item A Prolog-based consensus mechanism that performs weighted voting, disagreement detection, conflict resolution, and binary classification (benign vs.\ malignant) with explanation generation.
    \item A dynamic, per-case weight assignment mechanism that adjusts agent reliability weights based on the information richness of the available radiology images and pathology reports.
    \item A multi-factor NLP-richness scoring function that combines MeSH metadata, entity detection, negation awareness, and structural completeness to select evaluation cases where the NLP agents can perform meaningful extraction.
    \item An anatomically-calibrated size estimation method for the rule-based radiologist that uses blob detection with a chest X-ray field-of-view model, replacing a na\"ive pixel-dimension heuristic that produced clinically implausible values.
    \item Explicit handling of unknown measurements: agents return \texttt{None} when no measurement is detected and the consensus engine reduces the weight of agents that could not determine nodule size.
    \item A retrospective base-weight adjustment heuristic that nudges agent weights based on diagnostic feedback, allowing the system to gradually favour empirically more reliable agents over time.
    \item An end-to-end evaluation on real clinical reports using NLP-derived binary ground truth for system-level classification.
    \item A systematic ablation and claim verification framework with seven mandatory baselines, four ablation categories (agent, weighting, symbolic layer, NLP component), k-fold cross-validation, and an automated \texttt{ClaimVerifier} that produces PASS/FAIL verdicts for each architectural hypothesis.
    \item A Streamlit-based user interface with explainability features, including dynamic weight visualization, an agent thinking process trace that exposes the BDI reasoning loop (perception, deliberation, intention), and an evaluation dashboard for system-level performance monitoring.
\end{enumerate}


% =============================================================================
% BACKGROUND
% =============================================================================
\section{Background and Related Work}
\label{sec:background}

\subsection{NLP in Radiology}
\label{subsec:nlp-radiology}

Pons et al.~\cite{pons2016nlpradiology} present a systematic review of NLP applications in radiology, describing a complete pipeline consisting of: (i) report section splitting (e.g., separating Findings from Impression); (ii) tokenization and normalization (handling abbreviations and standardizing measurements); (iii) syntax and semantic processing; and (iv) application-specific extraction. Systems like CheXpert~\cite{irvin2019chexpert} and NegBio~\cite{peng2018negbio} have demonstrated that NLP can achieve high accuracy in this domain without the need for large annotated datasets required by deep learning models. They emphasize that \emph{negation detection} is essential for clinical NLP, since a large proportion of medical concepts in radiology reports appear in negated contexts (e.g., ``no classification of nodule''). My NLP pipeline follows this structure, with each stage implemented across three specialized Pathologist agents (Section~\ref{subsubsec:pathologists}).

\subsection{Negation and Uncertainty Detection}
\label{subsec:negation}

Chapman et al.~\cite{chapman2001negex} introduced NegEx, a simple algorithm for identifying negated findings in clinical text. The algorithm operates by: (1) detecting \emph{trigger phrases} (e.g., ``no'', ``without'', ``denies'', ``ruled out''); (2) defining a forward or backward \emph{scope window} (typically 5--6 words); and (3) marking any medical entity within that scope as negated. Harkema et al.~\cite{harkema2009context} extended this approach with ConText, adding support for temporal status.

For uncertainty detection, the CheXpert labeler~\cite{irvin2019chexpert} demonstrated that rule-based approaches can classify radiology report mentions as positive, negative, or uncertain using trigger words such as ``possible'', ``cannot exclude'', ``may represent'', and ``suggestive of''. While CheXpert targets 14 chest X-ray observations, my project applies the same principles to the narrower task of nodule-specific attribute extraction.

\subsection{BDI Multi-Agent Systems}
\label{subsec:bdi}

The system is organized as a \emph{Belief--Desire--Intention (BDI) multi-agent system}~\cite{rao1995bdi}, adapted for the Lung-Nodule MAS. In this context, \textbf{Beliefs} represent the specific diagnostic classification held by an agent (e.g., a Radiologist believing a nodule is present with probability $p$, or a Pathologist believing a finding is negated). \textbf{Desires} represent the clinical objective to process a specific patient case (e.g., the goal \texttt{!analyze\_case}). \textbf{Intentions} are the runtime execution of image analysis models or NLP pipelines chosen to fulfill those goals. This architecture is appropriate for medical decision support because it mirrors the clinical workflow: independent specialists (radiologists, pathologists) generate findings that are synthesized by a coordinating physician (oncologist) into a final recommendation.

This architecture is implemented using the SPADE-BDI framework~\cite{spadebdi_docs}, which extends the SPADE middleware~\cite{palanca2020spade} to support AgentSpeak(L) plans. The use of AgentSpeak(L) for defining agent reasoning follows the standard approach for programming BDI agents popularized by Jason~\cite{bordini2009multiagentprogramming}. In my implementation, \href{https://github.com/sepehrkdi/lung_nodule_mas/blob/main/spade_main.py}{\texttt{spade\_main.py}} serves as the entry point, initializing specialized BDI agents that execute strict AgentSpeak logic defined in `.asl` files.

\subsection{Multi-Agent Consensus}
\label{subsec:consensus-bg}

Collective intelligence theory suggests that aggregating independent opinions often outperforms individual experts. Wolf et al.~\cite{wolf2015collective} demonstrated that pooling the decisions of multiple radiologists significantly improves diagnostic accuracy (both sensitivity and specificity). Similarly, calculating consensus from double reading is a standard quality assurance practice in radiology~\cite{schaefer2018double}. My system automates this ``double reading'' by using multiple agents (both CV and NLP) and resolving their conflicts via a structured voting mechanism.

\subsection{Neuro-Symbolic AI}
\label{subsec:neurosymbolic-bg}

Neuro-symbolic AI seeks to combine the learning capability of neural networks with the reasoning transparency of symbolic logic~\cite{garcez2019neurosymbolic}. In high-stakes domains like medicine, pure black-box models are often insufficient due to lack of explainability. Recent systems like ProCDS~\cite{tan2025procds} have shown that using Prolog to enforce logical consistency on top of statistical model outputs can reduce errors and provide clinically meaningful explanations. The current project also adopts this neuro-symbolic approach: CNNs and NLP agents provide perception, while Prolog provides reasoning.

\subsection{Medical Decision Standards}
\label{subsec:medical-standards}

The Oncologist agent's reasoning layer encodes decision criteria grounded in two publicly documented medical standards:

\begin{itemize}
    \item \textbf{Lung-RADS v1.1}~\cite{acr2019lungrads}: The American College of Radiology's Lung Imaging Reporting and Data System defines categories from 1 (Negative) to 4X (Highly Suspicious) based on nodule size, texture, and morphological features. This provides a clear, codifiable mapping from nodule classification to management recommendations.
    \item \textbf{TNM Staging (8th Edition)}~\cite{ajcc2017stagingmanual8}: The AJCC TNM classification provides tumor staging based on tumor size (T), lymph node involvement (N), and distant metastasis (M). I reference the staging thresholds to ensure the rule base uses medically consistent vocabulary.
\end{itemize}


% =============================================================================
% SYSTEM ARCHITECTURE
% =============================================================================
\section{System Architecture}
\label{sec:architecture}

\subsection{Overview}
\label{subsec:arch-overview}

The system emulates the real-world clinical workflow in which independent specialists generate domain-specific reports that are subsequently integrated by a coordinating clinician to inform diagnosis and management:

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
    agent/.style={draw, rounded corners, minimum width=2.2cm, minimum height=1cm, align=center, font=\small},
    rads/.style={agent, fill=blue!15},
    paths/.style={agent, fill=green!15},
    onc/.style={agent, fill=orange!15},
    prolog/.style={agent, fill=yellow!15},
    arr/.style={-{Stealth[length=2.5mm]}, thick},
]
% Radiologists
\node[rads] (r1) at (-3.5, 3) {R1\\Conservative};
\node[rads] (r2) at (0, 3) {R2\\Balanced};
\node[rads] (r3) at (3.5, 3) {R3\\Sensitive};

% Pathologists
\node[paths] (p1) at (-3.5, -3) {P1\\Regex/Rules};
\node[paths] (p2) at (0, -3) {P2\\spaCy/NER};
\node[paths] (p3) at (3.5, -3) {P3\\NegEx/Context};

% Prolog
\node[prolog] (prolog) at (0, 0) {Prolog Consensus\\(Weighted Voting)};

% Oncologist
\node[onc] (onc) at (6, 0) {Oncologist\\(BDI)};

% Arrows
\draw[arr] (r1) -- (prolog);
\draw[arr] (r2) -- (prolog);
\draw[arr] (r3) -- (prolog);
\draw[arr] (p1) -- (prolog);
\draw[arr] (p2) -- (prolog);
\draw[arr] (p3) -- (prolog);
\draw[arr] (prolog) -- (onc);

% Input labels
\node[font=\footnotesize, above=0.1cm of r2] {X-ray Image};
\node[font=\footnotesize, below=0.1cm of p2] {Report Text};

% Output
\node[font=\footnotesize, right=0.1cm of onc] {\begin{tabular}{l}Decision +\\Explanation\end{tabular}};
\end{tikzpicture}
\caption{Multi-agent system architecture. Radiologist agents (R1--R3) process images; Pathologist agents (P1--P3) process report text. Before consensus, the orchestrator computes per-case dynamic weights from information richness (Section~\ref{subsec:dynamic-weights}). All findings and dynamic weights are fused by the Prolog consensus engine, and the Oncologist produces the final decision with explanation.}
\label{fig:architecture}
\end{figure}

\subsection{Agent Descriptions}
\label{subsec:agents}

\subsubsection{Radiologist Agents (R1--R3)}
\label{subsubsec:radiologists}

The system deploys three distinct Radiologist agents to simulate clinical inter-reader variability. The CNN-based agents (R1, R2) utilize the \texttt{TorchXRayVision} library~\cite{cohen2022torchxrayvision}, employing the DenseNet-121 architecture proposed in CheXNet~\cite{rajpurkar2017chexnet}. CheXNet has been shown to exceed effective radiologist performance on pneumonia detection tasks, providing a strong baseline for automated chest X-ray reading. The agents monitor the model's output probability for the ``Nodule'' or ``Mass'' pathology classes. No additional CV training is performed. I rely on these robust frozen weights.

\begin{table}[ht]
\centering
\begin{tabular}{llcl}
\toprule
\textbf{Agent} & \textbf{Style} & \textbf{Base Weight} & \textbf{Behavior} \\
\midrule
R1 & Conservative & 1.0 & High specificity, fewer false positives \\
R2 & Balanced     & 1.0 & Standard operating point \\
R3 & Rule-Based   & 0.7 & Size/texture Lung-RADS rules \\
\bottomrule
\end{tabular}
\caption{Radiologist agent configurations. Base weights are dynamically scaled per-case by the information-richness heuristic (Section~\ref{subsec:dynamic-weights}). The CNN outputs are post-processed with a temperature-scaled sigmoid calibration ($k=25.0$, $\mu=0.62$) to ensure improved probability spread across the [0,1] range.}
\label{tab:radiologists}
\end{table}

The rule-based radiologist (R3) applies Lung-RADS size/texture rules to estimate malignancy probability. Because the NLMCXR dataset does not include segmentation masks or size annotations, R3 must estimate nodule size from the image itself. A na\"ive approach of dividing the image's pixel dimension by a fixed constant (e.g., $\texttt{max(shape)}/5$) produces clinically implausible values (e.g., 102--125\,mm for a standard 512$\times$624 image), since pixel count does not correspond to physical size.

Instead, R3 implements an \emph{anatomically-calibrated blob detection} pipeline:
\begin{enumerate}
    \item An adaptive threshold is computed at $\mu + \sigma$ of the image's intensity distribution to isolate dense/bright regions.
    \item Connected component analysis (via \texttt{scipy.ndimage.label}) identifies candidate blobs in the thresholded binary image.
    % \item Each blob is filtered by: (a)~area ratio $\in [0.0002, 0.1]$ of total image area, (b)~bounding-box aspect ratio $> 0.3$ (excluding elongated structures), and (c)~mean intensity above the global mean.
    \item The largest qualifying blob's pixel-space diameter is converted to millimeters assuming a standard PA chest X-ray field of view:
    \begin{equation}
        d_\text{mm} = \frac{2\sqrt{A_\text{px} / \pi}}{H_\text{px}} \times 300
    \label{eq:blob-size}
    \end{equation}
    where $A_\text{px}$ is the blob area in pixels and $H_\text{px}$ is the image height.
    \item If no qualifying blob is found, \texttt{size\_mm} is set to \texttt{None} with \texttt{size\_source=``none\_detected''}, rather than a fabricated default.
\end{enumerate}

On real NLMCXR images (e.g., CXR1000), this produces size estimates of 5.1\,mm, 9.6\,mm, and 52.4\,mm for different views which are clinically plausible values compared to the 124.8\,mm produced by the old heuristic for every view.

\subsubsection{Pathologist Agents (P1--P3)}
\label{subsubsec:pathologists}

The Pathologist agents are the main NLP contribution, with three active agents analyzing textual evidence. The 3rd agent (Pathologist-3) uses context analysis to handle negation and uncertainty. It implements an extended NegEx algorithm \cite{chapman2001negex} integrated with ConText \cite{harkema2009context} logic. This agent identifies trigger phrases (e.g., ``no evidence of'', ``cannot exclude'') and determines their scope to assign certainty labels (Affirmed, Negated, or Uncertain) to extracted clinical entities. This prevents false positives from negated findings (e.g., ``no nodule'') and flags ambiguous cases for lower-confidence scoring. The term ``Pathologist'' is used metaphorically to represent agents that analyze textual evidence, similar to how pathologists extract findings from reports. Each agent implements a different NLP strategy:

\begin{table}[ht]
\centering
\begin{tabular}{llcl}
\toprule
\textbf{Agent} & \textbf{Approach} & \textbf{Base Weight} & \textbf{Focus} \\
\midrule
P1 & Regex/Rules  & 0.8 & Robust patterns, section-based extraction \\
P2 & spaCy/NER    & 0.9 & Dependency parsing, frame building \\
P3 & NegEx/Context & 0.85 & Negation and uncertainty detection \\
\bottomrule
\end{tabular}
\caption{Pathologist agent configurations. P2 employs multi-pass dependency traversal for long-distance attribute association (Section~\ref{subsec:dependency-frames}). Base weights are dynamically scaled per-case by the information-richness heuristic (Section~\ref{subsec:dynamic-weights}).}
\label{tab:pathologists}
\end{table}

All three Pathologist agents' size extraction methods return a \texttt{(size\_mm, size\_source)} tuple rather than a bare numeric value. When a measurement is found in the text, \texttt{size\_source} is set to \texttt{``regex''} (P1, P3) or \texttt{``spacy''} (P2). When no measurement is detected, the agents return \texttt{(None, ``unknown'')}. This design has three effects:
\begin{itemize}
    \item The malignancy-estimation heuristic in each Pathologist skips the size-based adjustment when \texttt{size\_mm} is \texttt{None}, relying only on textual descriptors (suspicious/benign terms, texture keywords).
    \item The \texttt{size\_source} tag is propagated into the agent's findings, making the provenance of every size value transparent.
    \item The consensus engine reduces the weight of any agent whose \texttt{size\_source} is \texttt{``unknown''} or \texttt{``none\_detected''} (Section~\ref{subsec:size-weight-reduction}).
\end{itemize}

\subsubsection{Oncologist Agent}
\label{subsubsec:oncologist}

The Oncologist agent integrates all outputs using SWI-Prolog (accessed via PySwip~\cite{pyswip_repo}). It implements:
\begin{enumerate}
    \item Dynamic weight computation: before each case, the Oncologist invokes a \texttt{DynamicWeightCalculator} (Section~\ref{subsec:dynamic-weights}) to produce per-case weights $\hat{w}_i$ from the base weights $w_i$ and the information richness of the available data.
    \item Weighted voting: $P_\text{final} = \frac{\sum_{i} \hat{w}_i \cdot c_i \cdot p_i}{\sum_{i} \hat{w}_i \cdot c_i}$ where $\hat{w}_i$ is the \emph{dynamic} per-case agent weight, $c_i$ the reported confidence, and $p_i$ the malignancy probability.
    \item Binary classification: The consensus probability is thresholded at 0.5 to produce a binary decision ($\leq 0.5$ benign, $> 0.5$ malignant).
    \item Conflict detection: disagreement is flagged when the standard deviation of agent probabilities exceeds 0.08 (see Section~\ref{subsec:disagreement}).
    \item Resolution strategies: (1) trust CNN radiologists when NLP agrees; (2) pathologist override when pathologist consensus is high-confidence ($P \ge 0.6$) but radiologist consensus is indeterminate; (3) rule-based agent as tiebreaker; (4) conservative recommendation under strong disagreement.
    \item Explanation Generation: a natural-language summary of which experts agreed/disagreed, the dynamic weight rationale, and which Prolog rule fired.
    \item Retrospective Weight Adjustment: a simple online heuristic that nudges agent base weights based on diagnostic feedback, gradually adapting to agent reliability over time (see Section~\ref{subsec:continual-learning}).
\end{enumerate}


% =============================================================================
% NLP PIPELINE
% =============================================================================
\section{NLP Pipeline}
\label{sec:nlp}

The NLP pipeline follows the radiology NLP architecture described by Pons et al.~\cite{pons2016nlpradiology}. The pipeline is distributed across the three Pathologist agents, with each agent implementing extraction strategies.

\subsection{Report Section Splitting}
\label{subsec:section-splitting}

Radiology reports in the Open-I collection follow a semi-structured format with standard section headers.
Different sections carry different semantic roles: the \texttt{Findings} section typically encodes the radiologistâ€™s key observations, while \texttt{Indication} and \texttt{Technique} primarily reflect exam context or procedural details.
Prior work in radiology NLP has shown that segmenting reports and treating sections according to their semantic content improves the extraction of clinically relevant information, as sections differ in their diagnostic value and purpose~\cite{casey2021nlpradiology, grey2021sectionsegmentation}.
To reflect these differences in my pipeline, I identify and segment reports into sections with the following weighting scheme:

\begin{lstlisting}[language=Python, caption={Section splitting with weighting.}]
SECTION_HEADERS = ["FINDINGS:", "INDICATION:",
                   "TECHNIQUE:", "COMPARISON:"]
SECTION_WEIGHTS = {"FINDINGS": 1.0,
                   "INDICATION": 0.5, "TECHNIQUE": 0.2}
\end{lstlisting}

Here the \texttt{Findings} section is given the highest weight as the primary source of diagnostic observations, while \texttt{Indication} and \texttt{Technique} are down-weighted to reduce emphasis on contextual or procedural text that is less informative for downstream NLP tasks.


\subsection{Tokenization and Normalization}
\label{subsec:tokenization}

Medical text requires specialized tokenization to handle:
\begin{itemize}
    \item \textbf{Abbreviations:} Common thoracic imaging abbreviations (e.g., RUL, GGO, CXR) are expanded using a dictionary derived from the RadLex ontology~\cite{langlotz2006radlex} to specific anatomical and pathological terms.
    \item \textbf{Measurement normalization:} Size mentions in different formats (``8 mm'', ``0.8 cm'', ``8mm'') are normalized to millimeters using regex patterns with unit conversion.
    \item \textbf{Hyphenated terms:} Medical compounds (``well-defined'', ``ground-glass'', ``part-solid'') are handled as single tokens.
\end{itemize}

The medical abbreviation dictionary (\texttt{nlp/extractor.py}) maps common abbreviations to their expanded forms:

\begin{lstlisting}[language=Python, caption={RadLex-derived abbreviation dictionary.}]
# Medical abbreviation dictionary based on RadLex (Langlotz, 2006)
RADLEX_COMMON_TERMS = {
    'RUL': 'right upper lobe',   'RML': 'right middle lobe',
    'RLL': 'right lower lobe',   'LUL': 'left upper lobe',
    'LLL': 'left lower lobe',    'GGO': 'ground-glass opacity',
    'GGN': 'ground-glass nodule', 'CT': 'computed tomography',
    'CXR': 'chest x-ray',        'PET': 'positron emission tomography',
    'LDCT': 'low-dose computed tomography',
    'SPN': 'solitary pulmonary nodule',
    'LAD': 'lymphadenopathy',    'PA': 'posteroanterior',
    'AP': 'anteroposterior',     'LN': 'lymph node',
}
\end{lstlisting}

Size normalization uses cascading regex patterns with automatic unit conversion:

\begin{lstlisting}[language=Python, caption={Size extraction and normalization patterns.}]
SIZE_PATTERNS = [
    r'(\d+\.?\d*)\s*(mm|millimeter)',      # "15 mm", "15mm"
    r'(\d+\.?\d*)\s*(cm|centimeter)',      # "1.5 cm" -> converted to 15mm
    r'(\d+\.?\d*)\s*x\s*\d+\.?\d*\s*(mm|cm)',  # "15 x 12 mm"
]

def _extract_size(self, text: str) -> Tuple[Optional[float], str]:
    for pattern in self.SIZE_PATTERNS:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            value = float(match.group(1))
            unit = match.group(2).lower()
            # Convert cm to mm
            if unit in ('cm', 'centimeter'):
                value *= 10
            return (value, 'regex')
    return (None, 'unknown')  # Provenance tracking
\end{lstlisting}

Since spaCy/scispaCy~\cite{neumann2019scispacy} is available (Pathologist-2), the pipeline uses its rule-based tokenizer with custom rules for medical text.

\subsection{Nodule Mention Detection}
\label{subsec:mention-detection}

Nodule mentions are detected using a lexicon-based approach combined with patterns. The anchor terms serve as triggers for frame extraction (\texttt{nlp/dependency\_parser.py}):

\begin{lstlisting}[language=Python, caption={Anchor terms for nodule detection.}]
# Trigger terms that start a frame (Fleischner Society terminology)
ANCHOR_TERMS = {
    "nodule", "nodules", "mass", "masses", 
    "lesion", "lesions", "opacity", "opacities",
    "density", "densities", "granuloma", "granulomas",
    "tumor", "neoplasm"
}

# Characterization terms describe findings, not standalone anchors
CHARACTERIZATION_TERMS = {
    "granuloma", "hamartoma", "carcinoma", "adenocarcinoma",
    "metastasis", "metastases", "lymphoma", "infection",
    "tuberculoma", "fungal", "malignancy", "benign",
    "malignant", "inflammatory", "fibrosis", "atelectasis"
}
\end{lstlisting}

The detection algorithm distinguishes between primary anchors and characterization terms:

\begin{lstlisting}[language=Python, caption={Anchor detection with characterization filtering.}]
def extract(self, doc) -> List[NoduleFinding]:
    findings = []
    for token in doc:
        if token.lemma_.lower() in self.ANCHOR_TERMS:
            # Skip if term is characterizing another finding
            if self._is_descriptive_characterization(token):
                continue
            finding = self._build_frame(token, doc)
            if finding:
                findings.append(finding)
    return findings
\end{lstlisting}

This lexicon is derived from standard radiological terminology defined by the Fleischner Society~\cite{hansell2008fleischner}, ensuring that the system targets clinically recognized descriptors for pulmonary nodules and masses.

Pathologist-2 supplements this with scispaCy's biomedical NER model, which can recognize medical entities not present in the lexicon. Furthermore, Pathologist-2 employs the dependency-anchored frame building module (Section~\ref{subsec:dependency-frames}) to strictly associate attributes with these detected entities.

\subsection{Attribute Extraction}
\label{subsec:attribute-extraction}

Four categories of attributes are extracted, forming the minimum sufficient set required by the Lung-RADS classification algorithm encoded in the Prolog knowledge base:

\begin{enumerate}
    \item \textbf{Anatomical location:} Essential for entity resolution in multi-nodule reports. Regex patterns target lobe references (``right upper lobe'', ``RUL''), positional terms (``subpleural'', ``perihilar''), and laterality.
    \item \textbf{Size mentions:} The primary determinant of Lung-RADS category. Multiple patterns capture formats including ``15 mm'', ``1.5 cm'', and dimensional notations (``15 $\times$ 12 mm''), with automatic unit normalization to millimeters. When no size pattern matches, the extraction returns \texttt{None} with a provenance tag \texttt{size\_source=``unknown''}, ensuring that downstream components can distinguish between measured and unmeasured sizes.
    \item \textbf{Multiplicity:} A categorical decision criterion in Lung-RADS. Detection includes plural nodule mentions (``multiple nodules'', ``bilateral'', ``several opacities'', numeric quantifiers such as ``3 nodules'').
    \item \textbf{Descriptors:} Modifiers that alter risk classification thresholds (e.g., solid vs. ground-glass). Keywords are grouped by clinical category.
\end{enumerate}

The keyword dictionaries used for descriptor extraction (\texttt{nlp/dependency\_parser.py} and \texttt{nlp/extractor.py}):

\begin{lstlisting}[language=Python, caption={Texture and location term dictionaries.}]
# Terms describing texture/type
TEXTURE_TERMS = {
    "solid": "solid",
    "ground-glass": "ground_glass", "ggo": "ground_glass",
    "part-solid": "part_solid", "subsolid": "part_solid",
    "mixed": "part_solid", "calcified": "calcified"
}

# Terms describing location
LOCATION_TERMS = {
    "upper": "upper", "middle": "middle", "lower": "lower",
    "right": "right", "left": "left", "bilateral": "bilateral",
    "lobe": "lobe", "lung": "lung", "apex": "apex",
    "base": "base", "hilum": "hilum", "hilus": "hilum",
    "rul": "right_upper_lobe", "lul": "left_upper_lobe",
    "rll": "right_lower_lobe", "lll": "left_lower_lobe",
    "rml": "right_middle_lobe"
}
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Margin and calcification keyword dictionaries.}]
# Margin keywords (clinically significant for malignancy risk)
MARGIN_KEYWORDS = {
    'well_defined': ['well-defined', 'sharp', 'smooth', 
                     'circumscribed'],
    'poorly_defined': ['poorly-defined', 'indistinct', 'ill-defined'],
    'spiculated': ['spiculated', 'spiculation', 'corona radiata', 
                   'stellate'],
    'lobulated': ['lobulated', 'lobulation', 'scalloped'],
}

# Calcification patterns (benign indicators)
CALCIFICATION_KEYWORDS = {
    'popcorn': ['popcorn', 'popcorn calcification'],
    'laminated': ['laminated', 'laminated calcification'],
    'central': ['central calcification', 'centrally calcified'],
    'eccentric': ['eccentric', 'peripheral calcification'],
    'absent': ['no calcification', 'non-calcified'],
}
\end{lstlisting}

\subsection{Dependency-Anchored Frame Association}
\label{subsec:dependency-frames}

A major limitation of flat attribute extraction is the ``bag-of-words'' problem in reports with multiple findings. For example, in the phrase ``\textit{A 5mm nodule in the right upper lobe and a 12mm mass in the left lower lobe},'' simple proximity-based or regex extraction might incorrectly associate ``12mm'' with the ``nodule'' or ``right upper lobe'' with the ``mass.''

To resolve this, I implemented a \textbf{Dependency-Anchored Frame Building} module within Pathologist-2 (spaCy). This module moves beyond surface-level pattern matching to utilize the grammatical structure of the sentence.

\subsubsection{Multi-Pass Traversal Architecture}

A key challenge is handling \emph{long-distance dependencies} in complex clinical constructions. For example, in ``\textit{A nodule, likely representing granuloma, measuring 5mm},'' the size ``5mm'' is syntactically distant from ``nodule'' due to intervening clausal modifiers. Standard subtree traversal fails to capture such relations because the measurement verb ``measuring'' attaches as an adverbial clause (\texttt{acl}) rather than a direct modifier.

To address this, I propose a \textbf{novel four-pass traversal} strategy designed for this project.

The key constants for long-distance dependency resolution (\texttt{nlp/dependency\_parser.py}):

\begin{lstlisting}[language=Python, caption={Clausal modifier dependencies and measurement verbs.}]
# Dependency relations that introduce clausal modifiers
CLAUSAL_MODIFIER_DEPS = {
    "acl",      # Clausal modifier: "nodule measuring 5mm"
    "relcl",    # Relative clause: "nodule which measures 5mm"
    "acl:relcl", # Combined tag in some models
    "appos",    # Appositive: "nodule, a granuloma"
    "advcl",    # Adverbial clause
}

# Verbs that commonly introduce measurements in radiology
MEASUREMENT_VERBS = {
    "measure", "measuring", "measures", "measured",
    "size", "sizing", "show", "showing",
    "demonstrate", "demonstrating", "reveal", "revealing",
    "represent", "representing", "appear", "appearing",
}

# Dependencies where term characterizes, not primary anchor
CHARACTERIZING_DEP_RELATIONS = {
    "pobj",   # "with granuloma" (prep object)
    "nmod",   # "consistent with granuloma" (scispaCy)
    "dobj",   # "representing granuloma" (direct object)
    "attr",   # "is granuloma" (predicate attribute)
    "appos",  # "nodule, granuloma" (appositive)
    "obl",    # oblique nominal (Universal Dependencies)
}
\end{lstlisting}

\begin{enumerate}
    \item \textbf{Pass 1: Direct Modifiers:} Standard BFS traversal of the anchor's immediate subtree, collecting adjectives (\texttt{amod}), numeric modifiers (\texttt{nummod}), and compound nouns (\texttt{compound}). Traversal blocks crossing into coordinate structures (\texttt{conj}, \texttt{cc}) to prevent attribute leakage between findings.
    
    \item \textbf{Pass 2: Clausal Modifiers:} The system identifies clausal dependents of the anchor with relations in $\{\texttt{acl}, \texttt{relcl}, \texttt{acl:relcl}, \texttt{appos}, \texttt{advcl}\}$ defined by the Universal Dependencies framework~\cite{nivre2016universaldependencies}. For each clause head, if the lemma matches a measurement verb (e.g., \textit{measure}, \textit{measuring}, \textit{show}) or characterization verb (e.g., \textit{represent}, \textit{suggest}, \textit{indicate}), the entire clause subtree is traversed to extract size values and characterization terms (e.g., \textit{granuloma}, \textit{malignancy}).
    
    \item \textbf{Pass 3: Participial Chain Scanning:} For constructions where participial modifiers span comma-separated phrases, a linear scan from the anchor token to the sentence boundary identifies participial verbs (\texttt{VBG} tags) and their associated measurements. This handles cases like ``\textit{nodule, possibly calcified, measuring 15mm, located in the LUL}.''
    
    \item \textbf{Pass 4: Appositive Fallback:} If no size was extracted in earlier passes, the system performs a sentence-wide scan for unclaimed measurements. A measurement is ``unclaimed'' if it does not belong to another anchor's subtree and is not associated with a characterization term that describes the current anchor.
\end{enumerate}

The multi-pass traversal is implemented in the \texttt{\_build\_frame} method:

\begin{lstlisting}[language=Python, caption={Multi-pass frame building algorithm.}]
def _build_frame(self, anchor: Token, doc) -> Optional[NoduleFinding]:
    finding = NoduleFinding(anchor_text=anchor.text, anchor_idx=anchor.i)
    visited = set()
    
    # PASS 1: Direct modifiers (amod, compound, nummod)
    for token in anchor.subtree:
        if token.dep_ in ("conj", "cc"):  # Block coordination
            break
        visited.add(token.i)
        self._analyze_modifier(token, finding, path="direct")
    
    # PASS 2: Clausal modifiers (acl, relcl, appos, advcl)
    for child in anchor.children:
        if child.dep_ in CLAUSAL_MODIFIER_DEPS:
            self._process_clausal_modifiers(child, finding, visited)
    
    # PASS 3: Linear participial chain scan
    self._process_participial_chain(anchor, doc, finding, visited)
    
    # PASS 4: Appositive fallback for unclaimed measurements
    if finding.size_mm is None:
        self._extract_from_appositive_measurements(
            anchor, doc, finding, visited)
    
    # PASS 5: Graded uncertainty quantification
    self._quantify_uncertainty(anchor, finding)
    
    return finding
\end{lstlisting}

\subsubsection{Characterization Term Handling}

Characterization terms (e.g., \textit{granuloma}, \textit{carcinoma}, \textit{metastasis}) present a disambiguation challenge: they appear in the anchor lexicon but often describe another finding rather than serving as primary anchors themselves. In ``\textit{consistent with granuloma},'' the term ``granuloma'' characterizes a nodule rather than introducing a new finding.

The system uses dependency relations to distinguish these cases. A characterization term is classified as \emph{descriptive} (not a primary anchor) if its dependency relation is in $\{\texttt{pobj}, \texttt{nmod}, \texttt{dobj}, \texttt{attr}, \texttt{appos}, \texttt{obl}\}$ and its head traces back to another anchor term within 5 dependency hops. This prevents fake frame creation while correctly extracting the characterization as an attribute of the parent finding.

\begin{lstlisting}[language=Python, caption={Characterization term disambiguation.}]
def _is_descriptive_characterization(self, token: Token) -> bool:
    """Check if anchor is characterizing another finding."""
    lemma = token.lemma_.lower()
    if lemma not in self.CHARACTERIZATION_TERMS:
        return False
    
    # Check if in characterizing dependency position
    if token.dep_ in self.CHARACTERIZING_DEP_RELATIONS:
        # Walk up tree to find another anchor
        head = token.head
        for _ in range(5):  # Max 5 hops
            if head.lemma_.lower() in self.ANCHOR_TERMS:
                return True  # Describes another finding
            if head == head.head:
                break
            head = head.head
    
    # Check "with/consistent with" patterns
    if token.head.lemma_ in ["with", "consistent", "represent"]:
        return True
    return False
\end{lstlisting}

\subsubsection{Structured Frame Generation}

The output is a list of structured \texttt{NoduleFinding} dataclass objects:

\begin{lstlisting}[language=Python, caption={NoduleFinding dataclass definition.}]
@dataclass
class NoduleFinding:
    """Structured representation of a single nodule finding."""
    anchor_text: str           # Word that triggered frame ("nodule")
    anchor_idx: int            # Token index of the anchor
    
    # Attributes linked via dependency parsing
    size_mm: Optional[float] = None
    size_source: str = "unknown"  # Provenance: "regex", "spacy", ...
    texture: Optional[str] = None
    location: Optional[str] = None
    margins: Optional[str] = None
    calcification: bool = False
    
    # Characterization from clausal modifiers
    characterization: Optional[str] = None  # e.g., "granuloma"
    
    # Context flags
    is_negated: bool = False
    is_uncertain: bool = False
    is_historical: bool = False  # e.g., "prior nodule"
    
    # Graded uncertainty (aleatory vs epistemic)
    uncertainty: Optional[UncertaintyQuantification] = None
    
    # Interpretability: which passes contributed attributes
    extraction_paths: List[str] = field(default_factory=list)
\end{lstlisting}

The \texttt{extraction\_paths} field provides interpretability by recording which traversal pass contributed each attribute (e.g., \texttt{[``acl:measure'', ``linear\_measurement:5'']}), supporting error analysis and debugging.

This structured approach allows the Pathologist agent to select the ``Index Nodule'', defined as the largest or most suspicious finding, for its primary report, while retaining the full structured data for conflict resolution.

\subsection{Negation Detection}
\label{subsec:negation-detection}

Pathologist-3 implements NegEx-style negation detection~\cite{chapman2001negex, harkema2009context} with the following components.

The \texttt{Trigger} dataclass encapsulates trigger metadata (\texttt{nlp/negation\_detector.py}):

\begin{lstlisting}[language=Python, caption={Trigger dataclass for NegEx.}]
@dataclass
class Trigger:
    """A trigger phrase for negation or uncertainty detection."""
    phrase: str           # e.g., "no evidence of"
    category: Certainty   # NEGATED or UNCERTAIN
    direction: str        # "forward", "backward", or "bidirectional"

class Certainty(Enum):
    AFFIRMED = "affirmed"   # Positively stated
    NEGATED = "negated"     # Explicitly denied
    UNCERTAIN = "uncertain" # Hedged or uncertain
\end{lstlisting}

\textbf{Trigger phrases} are categorized by scope direction:

\begin{lstlisting}[language=Python, caption={Negation trigger phrases (NegEx-based).}]
NEGATION_TRIGGERS = [
    # Pre-negation triggers (scope goes FORWARD)
    Trigger("no evidence of", Certainty.NEGATED, "forward"),
    Trigger("no", Certainty.NEGATED, "forward"),
    Trigger("without", Certainty.NEGATED, "forward"),
    Trigger("negative for", Certainty.NEGATED, "forward"),
    Trigger("denies", Certainty.NEGATED, "forward"),
    Trigger("absence of", Certainty.NEGATED, "forward"),
    Trigger("rules out", Certainty.NEGATED, "forward"),
    Trigger("unremarkable", Certainty.NEGATED, "forward"),
    Trigger("free of", Certainty.NEGATED, "forward"),
    Trigger("fails to reveal", Certainty.NEGATED, "forward"),
    
    # Post-negation triggers (scope goes BACKWARD)
    Trigger("is ruled out", Certainty.NEGATED, "backward"),
    Trigger("unlikely", Certainty.NEGATED, "backward"),
    Trigger("not seen", Certainty.NEGATED, "backward"),
    Trigger("not identified", Certainty.NEGATED, "backward"),
    Trigger("not demonstrated", Certainty.NEGATED, "backward"),
    Trigger("is absent", Certainty.NEGATED, "backward"),
]
\end{lstlisting}

\textbf{Scope determination:} After detecting a trigger, a window of up to 6 words is scanned in the indicated direction. Any nodule-related entity within this window is marked as negated. The scope is terminated early by \emph{termination terms} and sentence-ending punctuation:

\begin{lstlisting}[language=Python, caption={Scope termination terms.}]
SCOPE_TERMINATORS = [
    # Conjunctions that change context
    "but", "however", "although", "though", "except",
    "apart from", "nevertheless", "yet", "whereas",
    # Causal terms (change subject)
    "cause", "causing", "secondary to", "due to", "because",
    # Relative clauses
    "which", "who", "that", "whose",
    # Punctuation
    ".", ";", ":", "\n"
]
\end{lstlisting}

\textbf{Algorithm:} The NegEx detection follows a four-step process:
\begin{enumerate}
    \item Identify all trigger phrases and their positions in the text.
    \item For each trigger, determine scope boundaries (direction + window + terminators).
    \item For each detected entity, check whether it falls within any trigger's scope.
    \item If within a negation scope, label as \textsc{negated}; otherwise \textsc{affirmed}.
\end{enumerate}

\begin{lstlisting}[language=Python, caption={NegEx scope detection algorithm.}]
class NegExDetector:
    def __init__(self, scope_window: int = 6):  # Default from NegEx paper
        self.scope_window = scope_window
        self._triggers = NEGATION_TRIGGERS + UNCERTAINTY_TRIGGERS
        
    def _find_triggers(self, text: str) -> List[Tuple[Trigger, int, int]]:
        """Find all trigger phrases and their positions."""
        found = []
        text_lower = text.lower()
        for trigger in self._triggers:
            start = 0
            while True:
                pos = text_lower.find(trigger.phrase, start)
                if pos == -1:
                    break
                found.append((trigger, pos, pos + len(trigger.phrase)))
                start = pos + 1
        return sorted(found, key=lambda x: x[1])
    
    def _get_scope_boundary(self, text, trigger_end, direction):
        """Compute scope boundary considering window and terminators."""
        words_seen = 0
        pos = trigger_end if direction == "forward" else trigger_end
        while words_seen < self.scope_window:
            # Check for terminator
            for term in SCOPE_TERMINATORS:
                if text[pos:].lower().startswith(term):
                    return pos  # Early termination
            pos += 1 if direction == "forward" else -1
            if text[pos] == ' ':
                words_seen += 1
        return pos
\end{lstlisting}

\subsection{Uncertainty Detection}
\label{subsec:uncertainty-detection}

Uncertainty detection follows the same trigger-scope mechanism as negation, using a separate set of trigger phrases drawn from clinical NLP literature~\cite{irvin2019chexpert, harkema2009context}:

\begin{lstlisting}[language=Python, caption={Uncertainty trigger phrases (ConText-based).}]
UNCERTAINTY_TRIGGERS = [
    # Pre-uncertainty triggers (scope goes FORWARD)
    Trigger("possible", Certainty.UNCERTAIN, "forward"),
    Trigger("probable", Certainty.UNCERTAIN, "forward"),
    Trigger("may represent", Certainty.UNCERTAIN, "forward"),
    Trigger("might be", Certainty.UNCERTAIN, "forward"),
    Trigger("cannot exclude", Certainty.UNCERTAIN, "forward"),
    Trigger("cannot rule out", Certainty.UNCERTAIN, "forward"),
    Trigger("questionable", Certainty.UNCERTAIN, "forward"),
    Trigger("suspicious for", Certainty.UNCERTAIN, "forward"),
    Trigger("suggestive of", Certainty.UNCERTAIN, "forward"),
    Trigger("consistent with", Certainty.UNCERTAIN, "forward"),
    Trigger("differential includes", Certainty.UNCERTAIN, "forward"),
    Trigger("likely", Certainty.UNCERTAIN, "forward"),
    
    # Bidirectional triggers
    Trigger("versus", Certainty.UNCERTAIN, "bidirectional"),
    Trigger("vs", Certainty.UNCERTAIN, "bidirectional"),
    Trigger("or", Certainty.UNCERTAIN, "bidirectional"),
    
    # Post-uncertainty triggers (scope goes BACKWARD)
    Trigger("is suspected", Certainty.UNCERTAIN, "backward"),
    Trigger("is questionable", Certainty.UNCERTAIN, "backward"),
    Trigger("cannot be excluded", Certainty.UNCERTAIN, "backward"),
    Trigger("should be considered", Certainty.UNCERTAIN, "backward"),
    Trigger("is indeterminate", Certainty.UNCERTAIN, "backward"),
]
\end{lstlisting}

Entities within an uncertainty scope are labeled \textsc{uncertain}. When both a negation trigger and an uncertainty trigger apply to the same entity, negation takes precedence, following the convention in CheXpert~\cite{irvin2019chexpert}.

The final output for each entity is a three-valued certainty label: \textsc{affirmed}, \textsc{negated}, or \textsc{uncertain}. These labels are passed to the Oncologist agent, which uses them during conflict resolution (e.g., a negated nodule mention reduces the overall suspicion score).

\subsection{Graded Uncertainty Quantification}
\label{subsec:graded-uncertainty}

While categorical certainty labels (\textsc{affirmed}/\textsc{negated}/\textsc{uncertain}) are useful, they cannot distinguish between different \emph{sources} of uncertainty. To address this limitation, the system implements \textbf{graded uncertainty quantification} that separates:

\begin{itemize}
    \item \textbf{Aleatory uncertainty:} Inherent randomness or ambiguity in the source text itself, which cannot be reduced by gathering more data from the same source. Caused by hedge phrases (``may represent'', ``cannot exclude''), conflicting evidence markers (``however'', ``but''), and equivocal language.
    \item \textbf{Epistemic uncertainty:} Uncertainty due to incomplete knowledge or extraction, which can be reduced by better extraction or additional data. Caused by missing attributes (size, location), sparse extraction paths, and unknown measurement sources.
\end{itemize}

This distinction follows the uncertainty taxonomy of Der Kiureghian \& Ditlevsen~\cite{kiureghian2009aleatory}, adapted for clinical NLP. The specific weights and capping thresholds were empirically tuned for this domain for both aleatory and epistemic uncertainty.

\subsubsection{Uncertainty Scoring}

Each extracted \texttt{NoduleFinding} receives an \texttt{UncertaintyQuantification} object (\texttt{nlp/uncertainty\_quantification.py}):

\begin{lstlisting}[language=Python, caption={UncertaintyQuantification dataclass.}]
@dataclass
class UncertaintyQuantification:
    certainty_score: float = 1.0       # [0,1] confidence
    aleatory_uncertainty: float = 0.0  # Inherent ambiguity
    epistemic_uncertainty: float = 0.0 # Knowledge gaps
    categorical_label: CertaintyLabel = CertaintyLabel.AFFIRMED
    contributing_factors: List[str] = field(default_factory=list)
    hedge_count: int = 0
    negation_strength: float = 0.0
    
    @property
    def total_uncertainty(self) -> float:
        # Quadrature combination: sqrt(a^2 + e^2)
        combined = (self.aleatory_uncertainty**2 + 
                    self.epistemic_uncertainty**2)**0.5
        return min(1.0, combined)
    
    @property
    def uncertainty_type(self) -> str:
        if self.aleatory_uncertainty > self.epistemic_uncertainty + 0.1:
            return "aleatory_dominant"
        elif self.epistemic_uncertainty > self.aleatory_uncertainty + 0.1:
            return "epistemic_dominant"
        return "mixed"
\end{lstlisting}

The hedge phrase sets used for aleatory uncertainty computation:

\begin{lstlisting}[language=Python, caption={Hedge phrase sets for aleatory uncertainty.}]
# Strong hedges high aleatory uncertainty (weight: 0.3 each)
STRONG_HEDGES = {
    "may represent", "could represent", "possibly", "probable",
    "suggestive of", "suspicious for", "cannot exclude",
    "questionable", "equivocal", "indeterminate"
}

# Weak hedges moderate uncertainty (weight: 0.15 each)
WEAK_HEDGES = {
    "likely", "appears", "seems", "consistent with",
    "compatible with", "suggests", "indicating", "representing"
}

# Conflicting evidence markers (weight: 0.2 each)
CONFLICT_MARKERS = {
    "however", "although", "but", "nevertheless",
    "on the other hand", "alternatively", "versus", "vs"
}
\end{lstlisting}

\paragraph{Aleatory uncertainty} is computed from hedge phrase detection:
\begin{equation}
U_{\text{aleatory}} = \min\left(1.0,\; \sum_i c_i \cdot w_i\right)
\end{equation}
where $c_i$ is the count of triggers in category $i$ and $w_i$ is the weight (strong hedges: 0.3, weak hedges: 0.15, conflict markers: 0.2).

\begin{lstlisting}[language=Python, caption={Aleatory uncertainty computation.}]
def _compute_aleatory_uncertainty(self, text: str) -> Tuple[float, List[str]]:
    factors, uncertainty = [], 0.0
    
    # Strong hedges: +0.3 each (capped at 0.6)
    strong_matches = self._strong_hedge_pattern.findall(text)
    if strong_matches:
        uncertainty += min(0.6, len(strong_matches) * 0.3)
        factors.append(f"strong_hedges:{','.join(set(strong_matches))}")
    
    # Weak hedges: +0.15 each (capped at 0.3)
    weak_matches = self._weak_hedge_pattern.findall(text)
    if weak_matches:
        uncertainty += min(0.3, len(weak_matches) * 0.15)
        factors.append(f"weak_hedges:{','.join(set(weak_matches))}")
    
    # Conflict markers: +0.2 each (capped at 0.4)
    conflict_matches = self._conflict_pattern.findall(text)
    if conflict_matches:
        uncertainty += min(0.4, len(conflict_matches) * 0.2)
        factors.append(f"conflict_markers:{','.join(set(conflict_matches))}")
    
    return min(1.0, uncertainty), factors
\end{lstlisting}

\paragraph{Epistemic uncertainty} is computed from extraction completeness:
\begin{equation}
U_{\text{epistemic}} = \min\left(1.0,\; 0.15 \cdot |\text{missing\_attrs}| + 0.15 \cdot \mathbb{1}_{\text{size}=\text{None}} + 0.2 \cdot \max(0, 2 - |\text{paths}|)\right)
\end{equation}
where $|\text{missing\_attrs}|$ is the cardinality (count) of missing critical attributes from the expected set $\{\texttt{size\_mm}, \texttt{location}, \texttt{texture}, \texttt{margins}\}$, $\mathbb{1}_{\text{size}=\text{None}}$ is an indicator function (1 if size is unknown, 0 otherwise) that applies an additional penalty for missing size measurements, and $|\text{paths}|$ is the number of extraction paths used to construct the frame. The third term, $\max(0, 2 - |\text{paths}|)$, penalizes sparse extraction: +0.2 for each path below the minimum threshold of 2. The entire sum is capped at 1.0.

\begin{lstlisting}[language=Python, caption={Epistemic uncertainty computation.}]
# Expected attributes for complete extraction
EXPECTED_ATTRIBUTES = {"size_mm", "location", "texture", "margins"}
MIN_EXTRACTION_PATHS = 2  # Minimum for low epistemic uncertainty

def _compute_epistemic_uncertainty(
    self, attributes: Dict[str, Any], extraction_paths: List[str]
) -> Tuple[float, List[str]]:
    factors, uncertainty = [], 0.0
    
    # Missing critical attributes: +0.15 each (capped at 0.6)
    missing_attrs = [attr for attr in EXPECTED_ATTRIBUTES 
                     if attributes.get(attr) is None]
    if missing_attrs:
        uncertainty += min(0.6, len(missing_attrs) * 0.15)
        factors.append(f"missing_attrs:{','.join(missing_attrs)}")
    
    # Size unknown: +0.15 extra penalty
    if attributes.get("size_mm") is None:
        uncertainty += 0.15
        factors.append("size_unknown")
    
    # Few extraction paths: +0.2 x (MIN - actual)
    if len(extraction_paths) < MIN_EXTRACTION_PATHS:
        path_penalty = 0.2 * (MIN_EXTRACTION_PATHS - len(extraction_paths))
        uncertainty += path_penalty
        factors.append(f"sparse_paths:{len(extraction_paths)}")
    
    return min(1.0, uncertainty), factors
\end{lstlisting}

\paragraph{Total uncertainty} combines both sources via quadrature:
\begin{equation}
U_{\text{total}} = \min\left(1.0,\; \sqrt{U_{\text{aleatory}}^2 + U_{\text{epistemic}}^2}\right)
\end{equation}

This formulation reflects that the two uncertainty types are independent: a finding can have high aleatory uncertainty (ambiguous text) but low epistemic uncertainty (complete extraction), or vice versa.

\subsubsection{Uncertainty Type Classification}

The system classifies each finding's dominant uncertainty type:
\begin{itemize}
    \item \textbf{Aleatory-dominant}: $U_{\text{aleatory}} > U_{\text{epistemic}} + 0.1$ --- the text is ambiguous
    \item \textbf{Epistemic-dominant}: $U_{\text{epistemic}} > U_{\text{aleatory}} + 0.1$ --- extraction is incomplete
    \item \textbf{Mixed}: Neither dominates --- both contribute significantly
\end{itemize}

This classification informs downstream processing in subsequent steps.


% =============================================================================
% PROLOG CONSENSUS
% =============================================================================
\section{Prolog-Based Consensus Mechanism}
\label{sec:prolog}

The Oncologist agent uses SWI-Prolog, accessed from Python via PySwip~\cite{pyswip_repo}, to implement the consensus and decision logic. This neuro-symbolic integration aligns with modern reliable AI architectures~\cite{garcez2019neurosymbolic}, where symbolic logic verifies and explains sub-symbolic predictions. A dedicated \texttt{PrologEngine} class (\texttt{knowledge/prolog\_engine.py}) provides the Python--Prolog interface with the following capabilities:

\begin{itemize}
    \item \texttt{query\_lung\_rads(size, texture)}: Returns Lung-RADS category and management recommendation.
    \item \texttt{compute\_consensus(nodule\_id, findings)}: Computes weighted multi-agent consensus.
    \item \texttt{query\_tnm\_stage(nodule\_id, size)}: Returns TNM staging based on tumor characteristics.
\end{itemize}

The knowledge base is organized into two primary logic modules:

\begin{enumerate}
    \item \texttt{lung\_rads.pl}: Encodes the static domain knowledge, including Lung-RADS v1.1 classification rules, Fleischner Society guidelines, and TNM staging logic (Section~\ref{subsec:recommendations}). This module handles the \emph{clinical reasoning} (e.g., ``a 22mm solid nodule is T1c'').
    \item \texttt{multi\_agent\_consensus.pl}: Implements the dynamic social reasoning, including weighted voting, disagreement detection ($\sigma > 0.08$), and conflict resolution strategies (Section~\ref{subsec:disagreement}). This module handles the \emph{agent coordination} (e.g., ``Radiologist-1 and Pathologist-3 disagree, trigger conflict rule \#2'').
\end{enumerate}

\subsection{Agent Registry and Weighted Voting}
\label{subsec:voting}

The Prolog knowledge base defines agent types and their reliability weights (\texttt{knowledge/multi\_agent\_consensus.pl}):

\begin{lstlisting}[language=Prolog, caption={Agent type definitions and base weights.}]
% Agent type definitions
agent_type(radiologist_densenet, radiologist, cnn).
agent_type(radiologist_resnet, radiologist, cnn).
agent_type(radiologist_rulebased, radiologist, rule_based).
agent_type(pathologist_regex, pathologist, regex).
agent_type(pathologist_spacy, pathologist, nlp).
agent_type(pathologist_context, pathologist, context).

% Dynamic predicates asserted at runtime
:- dynamic agent_weight/2.
:- dynamic agent_finding/4.  % agent_finding(NoduleId, Agent, Prob, Class)

% Default weights (overridden by DynamicWeightCalculator)
agent_weight(radiologist_densenet, 1.0).
agent_weight(radiologist_resnet, 1.0).
agent_weight(radiologist_rulebased, 0.7).
agent_weight(pathologist_regex, 0.8).
agent_weight(pathologist_spacy, 0.9).
agent_weight(pathologist_context, 0.9).

% Weight accessor with default fallback
get_agent_weight(Agent, Weight) :-
    agent_weight(Agent, Weight), !.
get_agent_weight(_, 0.5).  % Default for unknown agents
\end{lstlisting}

The \texttt{:- dynamic} directive declares predicates that can be modified at runtime, allowing the orchestrator to retract default facts and assert per-case weights computed by the \texttt{DynamicWeightCalculator} (Section~\ref{subsec:dynamic-weights}).

The consensus probability is computed as:
\begin{equation}
P_\text{consensus} = \frac{\sum_{i=1}^{n} \hat{w}_i \cdot p_i}{\sum_{i=1}^{n} \hat{w}_i}
\label{eq:consensus}
\end{equation}

where $\hat{w}_i$ denotes the dynamic per-case weight for agent~$i$ (see Equation~\ref{eq:dynamic-weight}).

\begin{lstlisting}[language=Prolog, caption={Weighted consensus calculation predicate.}]
% Calculate weighted consensus from all agent findings
calculate_consensus(NoduleId, WeightedProb, Confidence) :-
    findall(
        prob(Agent, Prob, Weight),
        (agent_finding(NoduleId, Agent, Prob, _),
         get_agent_weight(Agent, Weight)),
        Findings
    ),
    Findings \= [],
    sum_weighted_probs(Findings, TotalWeightedProb, TotalWeight),
    TotalWeight > 0,
    WeightedProb is TotalWeightedProb / TotalWeight,
    calculate_agreement(Findings, WeightedProb, Confidence).

% Helper: Sum weighted probabilities
sum_weighted_probs([], 0, 0).
sum_weighted_probs([prob(_, Prob, Weight)|Rest], TotalProb, TotalWeight) :-
    sum_weighted_probs(Rest, RestProb, RestWeight),
    TotalProb is RestProb + (Prob * Weight),
    TotalWeight is RestWeight + Weight.

% Agreement/confidence based on variance: Confidence = max(0, 1 - 3*StdDev)
calculate_agreement(Findings, MeanProb, Confidence) :-
    length(Findings, N), N > 1,
    findall(Diff, (member(prob(_, Prob, _), Findings),
                   Diff is (Prob - MeanProb) ** 2), Diffs),
    sum_list(Diffs, SumDiffs),
    Variance is SumDiffs / N,
    StdDev is sqrt(Variance),
    Confidence is max(0, 1 - (StdDev * 3)).
\end{lstlisting}

Confidence is derived from inter-agent agreement: $\text{Confidence} = \max(0,\; 1 - 3\sigma)$, where $\sigma$ is the standard deviation of agent probabilities.

\subsection{Disagreement Detection and Resolution}
\label{subsec:disagreement}

Following the principles of collective intelligence~\cite{wolf2015collective}, the consensus engine applies a three-stage pipeline after computing the weighted average (Equation~\ref{eq:consensus}). Each stage uses \emph{different} thresholds that serve distinct purposes:

\paragraph{Stage 1: Disagreement Detection ($\sigma > 0.08$).}
The first stage determines whether agents disagree enough to warrant special handling. Disagreement is flagged when the standard deviation of agent probabilities exceeds~0.08. This threshold was empirically determined based on two factors:

\begin{itemize}
    \item \textbf{Sensitivity to moderate divergence:} For two groups of agents splitting opinions (e.g., Radiologists vs.\ Pathologists), the population standard deviation simplifies to $\sigma = \frac{|p - q|}{2}$. A threshold of 0.08 triggers when the probability gap exceeds~16\% (e.g., 0.42 vs.\ 0.68), ensuring that even subtle disagreements where one modality is uncertain and the other is weakly confident are flagged for resolution.
    \item \textbf{Safety margin for downstream rules:} A higher threshold (e.g., 0.15, requiring a 30\% gap) would be borderline for the conflict-resolution rules described below, risking missed triggers due to floating-point variations. The 0.08 threshold provides a comfortable margin.
\end{itemize}

\paragraph{Stage 2: Conflict Resolution (pattern-specific thresholds).}
If disagreement is detected, the system inspects the \emph{pattern} of the disagreement using stricter, rule-specific thresholds. The resolution strategies are inspired by multi-agent conflict resolution taxonomies~\cite{atoms2025conflictresolution}, focusing on voting and arbitration.

\begin{lstlisting}[language=Prolog, caption={Conflict resolution predicates.}]
% Disagreement detection: std dev > 0.08
has_disagreement(NoduleId) :-
    findall(Prob, agent_finding(NoduleId, _, Prob, _), Probs),
    length(Probs, N), N > 1,
    msort(Probs, Sorted),
    calculate_std_dev(Sorted, StdDev),
    StdDev > 0.08.

% Visual-Text Conflict: CNN malignant, NLP benign
resolve_disagreement(NoduleId, FinalProb, 0.4, 'visual_text_conflict') :-
    average_radiologist_prob(NoduleId, CNNProb), CNNProb > 0.65,
    average_pathologist_prob(NoduleId, PathProb), PathProb < 0.35,
    FinalProb is (CNNProb + PathProb) / 2.

% Text Override: NLP malignant, CNN benign (trust NLP)
resolve_disagreement(NoduleId, FinalProb, 0.8, 'text_override') :-
    average_radiologist_prob(NoduleId, CNNProb), CNNProb < 0.35,
    average_pathologist_prob(NoduleId, PathProb), PathProb > 0.65,
    FinalProb is PathProb.

% Pathologist Override: NLP confident, CNN indeterminate
resolve_disagreement(NoduleId, FinalProb, 0.7, 'pathologist_override') :-
    average_radiologist_prob(NoduleId, CNNProb),
    CNNProb >= 0.35, CNNProb =< 0.65,
    average_pathologist_prob(NoduleId, PathProb), PathProb >= 0.60,
    FinalProb is PathProb.

% Helper predicates
average_radiologist_prob(NoduleId, Avg) :-
    findall(P, (agent_finding(NoduleId, Agent, P, _),
                agent_type(Agent, radiologist, _)), Probs),
    sum_list(Probs, Sum), length(Probs, N), Avg is Sum / N.

average_pathologist_prob(NoduleId, Avg) :-
    findall(P, (agent_finding(NoduleId, Agent, P, _),
                agent_type(Agent, pathologist, _)), Probs),
    sum_list(Probs, Sum), length(Probs, N), Avg is Sum / N.
\end{lstlisting}

The following rules are evaluated in order:
\begin{enumerate}
    \item \textbf{Visual--Text Conflict ($P_\text{CV} > 0.65$ and $P_\text{NLP} < 0.35$):} CNN agents confidently predict malignancy while NLP agents confidently predict benign, indicating a visual false positive or a finding not yet recorded in the report. The system averages the probabilities, lowers confidence to~0.4, and triggers a ``Radiology Review Required'' recommendation.
    \item \textbf{Text Override ($P_\text{NLP} > 0.65$ and $P_\text{CV} < 0.35$):} NLP agents confidently detect malignancy but CNN agents miss it, suggesting a visual false negative. The system overrides with the Pathologist's probability and assigns high confidence.
    \item \textbf{Pathologist Override ($P_\text{NLP} \ge 0.60$ and $0.35 \le P_\text{CV} \le 0.65$):} Pathologists detect malignancy while Radiologists are indeterminate. The system uses the Pathologist's probability to prevent dilution of strong textual evidence by uncertain imaging models.
    \item \textbf{CNN--NLP Agreement ($|P_\text{CNN} - P_\text{NLP}| < 0.2$):} Both modalities reach similar conclusions; their weighted combination is used with 60/40 weighting.
    \item \textbf{Rule-Based Tiebreaker:} If CNN models disagree among themselves, the rule-based radiologist is used as a tiebreaker.
    \item \textbf{Conservative Default:} Under strong disagreement not covered by the rules above, the system defaults to a conservative recommendation and flags the case.
\end{enumerate}

Note that the 0.65/0.35 thresholds in the conflict rules above are \emph{not} the same as the 0.08 disagreement-detection threshold: Stage~1 asks ``\emph{do agents disagree at all?}'' (a low bar), while Stage~2 asks ``\emph{which specific conflict pattern applies?}'' (stricter, rule-specific conditions).

\paragraph{Stage 3: Binary Classification (threshold at 0.5).}
After resolution, the final consensus probability $P_\text{consensus}$ (whether from the weighted average or from a resolution override) is thresholded at~0.5: benign if $P_\text{consensus} < 0.5$, malignant if $P_\text{consensus} \ge 0.5$. Additionally, the engine assigns a Lung-RADS category and TNM stage as \emph{supplementary clinical recommendations} when applicable,these are informational outputs for clinical context and are not used as the primary classification labels.

\subsection{Size-Source Weight Reduction}
\label{subsec:size-weight-reduction}

Nodule size is a critical input to the Lung-RADS rules and the malignancy estimation heuristic. However, not every agent can reliably determine size for every case: the Pathologist agents may encounter reports without explicit measurements, and the rule-based Radiologist may find no blob in the image. In such cases, the agent's \texttt{size\_source} field is set to \texttt{``unknown''} (Pathologists) or \texttt{``none\_detected''} (Radiologist R3), and \texttt{size\_mm} is \texttt{None}.

To prevent these agents from having outsized influence on the consensus when they lack size information, the Prolog consensus engine applies a \emph{size-source weight penalty} before computing $P_\text{consensus}$:

\begin{equation}
\hat{w}_i^{\prime} =
\begin{cases}
0.5 \cdot \hat{w}_i & \text{if } \texttt{size\_source}_i \in \{\text{``unknown'', ``none\_detected''}\} \text{ or } \texttt{size\_mm}_i = \texttt{None}, \\
\hat{w}_i & \text{otherwise.}
\end{cases}
\label{eq:size-penalty}
\end{equation}

This 50\% reduction is applied \emph{after} the dynamic per-case weight scaling (Equation~\ref{eq:dynamic-weight}), so the final effective weight incorporates both the information-richness scaling and the size-provenance penalty. The rationale is that an agent whose malignancy estimate is not grounded in an actual size value provides a less reliable signal than one that has measured or extracted a real measurement.

\subsection{BDI Implementation Details}
\label{subsec:bdi-details}

The agent logic relies on a dual-layer belief system that integrates Python-based perception modules with the BDI reasoning engine.

\subsubsection{Belief Storage}

Beliefs are maintained in two synchronized forms. The \texttt{Belief} dataclass (\texttt{agents/spade\_base.py}) encapsulates individual beliefs:

\begin{lstlisting}[language=Python, caption={Belief dataclass for BDI agents.}]
@dataclass
class Belief:
    """A single belief held by an agent."""
    functor: str           # Predicate name, e.g., "classification"
    args: tuple            # Arguments, e.g., ("nodule_001", 0.85)
    annotations: Dict[str, Any] = field(default_factory=dict)
    
    def to_asl(self) -> str:
        """Convert to AgentSpeak literal format."""
        args_str = ', '.join(str(a) for a in self.args)
        base = f"{self.functor}({args_str})"
        if self.annotations:
            ann_strs = [f"{k}({v})" for k, v in self.annotations.items()]
            base += "[" + ", ".join(ann_strs) + "]"
        return base  # e.g., "classification(nodule_001, 0.85)[source(self)]"
\end{lstlisting}

\begin{itemize}
    \item \textbf{Python Layer:} Each agent (inheriting from \texttt{MedicalAgentBase}) maintains a list of \texttt{Belief} objects. Each object encapsulates a predicate (functor), arguments, and metadata such as the information source.
    \item \textbf{BDI Layer:} When the \texttt{spade\_bdi} runtime is active, these Python objects are serialized into AgentSpeak syntax and synchronized with the internal belief base, making them accessible to logical plans.
\end{itemize}

The abstract \texttt{MedicalAgentBase} class provides the foundation for all agents:

\begin{lstlisting}[language=Python, caption={MedicalAgentBase abstract class.}]
class MedicalAgentBase(ABC):
    """Abstract base class for all medical agents."""
    
    def __init__(self, name: str, asl_file: Optional[str] = None):
        self.name = name
        self.beliefs: List[Belief] = []
        self.internal_actions: Dict[str, Callable] = {}
        self._register_actions()
    
    def add_belief(self, functor: str, *args, **annotations):
        belief = Belief(functor, args, annotations)
        self.beliefs.append(belief)
        return belief
    
    @abstractmethod
    def _register_actions(self) -> None:
        """Register internal actions for AgentSpeak plans."""
        pass
    
    @abstractmethod
    async def process_request(
        self, request: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Process an analysis request and return findings."""
        pass
\end{lstlisting}

\subsubsection{Belief Updates}
Beliefs accumulate through two primary channels:
\begin{enumerate}
    \item \textbf{Communication (Passive):} Incoming messages with the \texttt{inform} performative are automatically parsed by the underlying SPADE handler, which extracts the content and asserts it as a new belief.
    \item \textbf{Active Perception (Internal Actions):} Unlike traditional BDI systems that rely on passive environment sensing, this system employs active perception. AgentSpeak plans execute internal actions which process raw data and inject new beliefs using \texttt{self.add\_belief()}.
\end{enumerate}

\subsubsection{Goal Structures and Conditional Execution}
The ASL plans (e.g., \texttt{asl/radiologist.asl}, \texttt{asl/oncologist.asl}) demonstrate goal management:
\begin{itemize}
    \item \textbf{Conditional Goals:} Plans utilize context conditions to guard execution. For example, the plan for \texttt{+!analyze} checks \texttt{: model\_loaded(false)} to trigger a repair goal (\texttt{!initialize}) before proceeding with the main task.
    \item \textbf{Data Dependencies:} The Oncologist agent uses \texttt{waiting\_for} beliefs to block execution until all required inputs (radiology and pathology findings) are received.
    \item \textbf{Re-evaluation:} Reactive plans monitor belief updates (e.g., changes in sensitivity thresholds) to trigger re-evaluation of previous conclusions.
\end{itemize}

\subsection{Dynamic Per-Case Weight Assignment}
\label{subsec:dynamic-weights}

A key limitation of static agent weights is the implicit assumption that every case offers the same quality and quantity of information in both modalities. In practice, some cases are accompanied by multiple high-quality X-ray projections but only a short report, while others contain detailed reports but limited or low-quality imaging. Static weights cannot adapt to this per-case asymmetry.

To address this, I replace the fixed weights with a \emph{dynamic, per-case weight assignment} based on an information-richness heuristic. The mechanism is implemented in a dedicated \texttt{DynamicWeightCalculator} module (\texttt{models/dynamic\_weights.py}), which serves as the \emph{single source of truth} for all agent weights.

\begin{lstlisting}[language=Python, caption={Base weights as single source of truth.}]
# models/dynamic_weights.py Single Source of Truth
BASE_WEIGHTS: Dict[str, float] = {
    # Radiologists
    "radiologist_densenet": 1.0,   # CNN strong visual classifier
    "radiologist_resnet": 1.0,     # CNN strong visual classifier
    "radiologist_rulebased": 0.7,  # Heuristic simpler but interpretable
    # Pathologists
    "pathologist_regex": 0.8,      # Regex shallow but fast
    "pathologist_spacy": 0.9,      # NLP/NER more robust
    "pathologist_context": 0.9,    # Negation/uncertainty clinical value
}
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={RichnessScores dataclass for per-case information metrics.}]
@dataclass
class RichnessScores:
    """
    Per-case information richness scores for each modality.
    Captures how much usable information the case provides.
    """
    radiology_richness: float = 0.5  # 0 = no/poor images, 1 = high-quality
    pathology_richness: float = 0.5  # 0 = sparse report, 1 = detailed

    # Sub-component breakdown for auditability
    radiology_components: Dict[str, float] = field(default_factory=dict)
    pathology_components: Dict[str, float] = field(default_factory=dict)
\end{lstlisting}

\subsubsection{Radiology Richness Score}

For each case, a radiology richness score $R_\text{rad} \in [0,1]$ is computed as a weighted combination of three sub-components:

\begin{equation}
R_\text{rad} = \alpha_1 \cdot S_\text{count} + \alpha_2 \cdot S_\text{PA} + \alpha_3 \cdot S_\text{quality}
\label{eq:rad-richness}
\end{equation}

where $(\alpha_1, \alpha_2, \alpha_3) = (0.35, 0.35, 0.30)$ and:

\begin{lstlisting}[language=Python, caption={DynamicWeightCalculator component weights.}]
class DynamicWeightCalculator:
    # Sub-component weights for radiology richness
    RAD_WEIGHT_NUM_IMAGES = 0.35     # More views -> more information
    RAD_WEIGHT_PA_VIEW = 0.35        # PA is most informative projection
    RAD_WEIGHT_IMAGE_QUALITY = 0.30  # Higher quality -> reliable features

    # Sub-component weights for pathology richness
    PATH_WEIGHT_REPORT_LENGTH = 0.25     # Longer reports have more detail
    PATH_WEIGHT_ENTITY_COUNT = 0.30      # More extracted entities -> richer
    PATH_WEIGHT_SECTION_COMPLETENESS = 0.20  # More sections -> better coverage
    PATH_WEIGHT_CERTAINTY = 0.25         # More affirmed mentions -> confidence

    # Scaling bounds: weight = base * (FLOOR + (1-FLOOR) * richness)
    SCALE_FLOOR = 0.5   # Minimum fraction of base weight
\end{lstlisting}

\begin{itemize}
    \item $S_\text{count}$: Image count score --- 0 images $\to$ 0, 1 image $\to$ 0.5, 2+ images $\to$ 0.8--1.0 (diminishing returns).
    \item $S_\text{PA}$: PA view presence --- 1.0 if a posterior-anterior or frontal view is available (the most diagnostically informative projection for lung nodules), 0.3 otherwise.
    \item $S_\text{quality}$: Image quality proxy --- estimated from image resolution ($\text{height} \times \text{width}$ normalized by a $512 \times 512$ reference), averaged across available views.
\end{itemize}

\subsubsection{Pathology Richness Score}

A pathology richness score $R_\text{path} \in [0,1]$ is computed from four textual sub-components:

\begin{equation}
R_\text{path} = \beta_1 \cdot S_\text{length} + \beta_2 \cdot S_\text{entities} + \beta_3 \cdot S_\text{sections} + \beta_4 \cdot S_\text{certainty}
\label{eq:path-richness}
\end{equation}

where $(\beta_1, \beta_2, \beta_3, \beta_4) = (0.25, 0.30, 0.20, 0.25)$ and:

\begin{itemize}
    \item $S_\text{length}$: Report text length --- character count of FINDINGS, mapped to $[0,1]$ with saturation at 300 characters.
    \item $S_\text{entities}$: NLP entity count --- number of medical entities and measurements extracted by the pathologist agents, normalized to $[0,1]$ with saturation at 5 entities.
    \item $S_\text{sections}$: Section completeness --- fraction of key report sections (FINDINGS, IMPRESSION, INDICATION) that are non-empty; e.g., all three present $\to$ 1.0.
    \item $S_\text{certainty}$: Certainty signal --- proportion of \textsc{affirmed} mentions relative to total (affirmed + negated + uncertain); a case dominated by negated findings receives a lower certainty score, reflecting reduced pathological informativeness.
\end{itemize}

\subsubsection{Weight Scaling Formula}

Given the richness scores, each agent's \emph{base weight} $w_i$ is scaled to its \emph{dynamic weight} $\hat{w}_i$ as follows:

\begin{equation}
\hat{w}_i =
\begin{cases}
w_i \cdot \bigl(\lambda + (1 - \lambda) \cdot R_\text{rad}\bigr) & \text{if agent $i$ is a radiologist,} \\
w_i \cdot \bigl(\lambda + (1 - \lambda) \cdot R_\text{path}\bigr) & \text{if agent $i$ is a pathologist,}
\end{cases}
\label{eq:dynamic-weight}
\end{equation}

where $\lambda = 0.5$ is a \emph{scale floor} parameter ensuring that $\hat{w}_i \in [0.5 \cdot w_i,\; w_i]$. The floor guarantees that no agent is ever silenced entirely: even when a modality's data is minimal, the corresponding agents still contribute half their base influence.

\begin{lstlisting}[language=Python, caption={Dynamic weight scaling implementation.}]
def compute_weights(self, case_metadata: Dict[str, Any]) -> tuple:
    """Compute dynamic per-agent weights for a single case."""
    base_weights = self._load_weights()
    richness = self._compute_richness(case_metadata)

    # Scale each agent's weight based on modality richness
    dynamic_weights = {}
    for agent_name, base_w in base_weights.items():
        agent_type = self._get_agent_type(agent_name)
        if agent_type == "radiologist":
            scale = self.SCALE_FLOOR + (1-self.SCALE_FLOOR) * richness.radiology_richness
        elif agent_type == "pathologist":
            scale = self.SCALE_FLOOR + (1-self.SCALE_FLOOR) * richness.pathology_richness
        else:
            scale = 1.0  # Unknown type -> no scaling
        dynamic_weights[agent_name] = round(base_w * scale, 4)
    return dynamic_weights, rationale
\end{lstlisting}

\subsubsection{Integration with Prolog}

Before computing consensus for each case, the orchestrator:
\begin{enumerate}
    \item Invokes \texttt{DynamicWeightCalculator.compute\_weights()} with the case metadata.
    \item Calls \texttt{retractall(agent\_weight(Agent, \_))} followed by \texttt{assertz(agent\_weight(Agent, $\hat{w}_i$))} for each agent, overriding the default Prolog facts with the per-case values.
    \item Proceeds with \texttt{calculate\_consensus/3}, which now retrieves the dynamically asserted weights via \texttt{get\_agent\_weight/2}.
\end{enumerate}

This ensures that both the Python-side weighted aggregation and the Prolog-side symbolic reasoning operate on identical, case-specific weights.

\subsubsection{Illustrative Example}

Table~\ref{tab:dynamic-weights-example} shows the dynamic weight computation for three NLMCXR cases that differ in their information profiles.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{2}{c}{\textbf{CXR1}} & \multicolumn{2}{c}{\textbf{CXR10}} & \multicolumn{2}{c}{\textbf{CXR100}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& $\hat{w}$ & Scale & $\hat{w}$ & Scale & $\hat{w}$ & Scale \\
\midrule
$R_\text{rad}$ & \multicolumn{2}{c}{0.728} & \multicolumn{2}{c}{0.728} & \multicolumn{2}{c}{0.728} \\
$R_\text{path}$ & \multicolumn{2}{c}{\cellcolor{yellow!25}0.882} & \multicolumn{2}{c}{\cellcolor{green!20}0.925} & \multicolumn{2}{c}{\cellcolor{red!15}0.712} \\
\midrule
R1 (DenseNet) & 0.864 & 0.864 & 0.864 & 0.864 & 0.864 & 0.864 \\
R2 (ResNet)   & 0.864 & 0.864 & 0.864 & 0.864 & 0.864 & 0.864 \\
R3 (Rules)    & 0.605 & 0.864 & 0.605 & 0.864 & 0.605 & 0.864 \\
P1 (Regex)    & \cellcolor{yellow!25}0.753 & 0.941 & \cellcolor{green!20}0.770 & 0.963 & \cellcolor{red!15}0.685 & 0.856 \\
P2 (spaCy)    & \cellcolor{yellow!25}0.847 & 0.941 & \cellcolor{green!20}0.866 & 0.963 & \cellcolor{red!15}0.770 & 0.856 \\
P3 (Context)  & \cellcolor{yellow!25}0.800 & 0.941 & \cellcolor{green!20}0.819 & 0.963 & \cellcolor{red!15}0.728 & 0.856 \\
\bottomrule
\end{tabular}
\caption{Dynamic weight scaling on three NLMCXR cases. $\hat{w}$ is the final dynamic weight; Scale = $\hat{w}/w_i$. Radiology richness is identical (all cases have 2 images including a PA view), whereas pathology richness varies with report detail, producing different pathologist weights per case.}
\label{tab:dynamic-weights-example}
\end{table}

\subsection{Retrospective Base-Weight Adjustment}
\label{subsec:continual-learning}

While the dynamic weight calculator (Section~\ref{subsec:dynamic-weights}) adapts agent influence on a per-case basis, the system also includes a simple \emph{retrospective base-weight adjustment} that tracks long-term agent reliability. This mechanism is not continual learning in the machine-learning sense (i.e., incremental model retraining on non-stationary data with forgetting mitigation); rather, it is an online heuristic that nudges base weights toward agents whose predictions more frequently agree with confirmed diagnoses.

After a definitive diagnosis is confirmed, the system updates the base weights stored in \texttt{data/learned\_weights.json} using a fixed-step reward/penalty rule:
\begin{equation}
w_i^{(t+1)} = \text{clamp}\left(w_i^{(t)} + \eta \cdot \delta_i,\; w_{\min},\; w_{\max}\right)
\label{eq:continual-learning}
\end{equation}
where $\eta=0.01$ is a step size, and $\delta_i = +1$ if agent $i$ correctly predicted the confirmed binary label, or $-1$ if it was incorrect. The weights are clamped to $[0.2, 3.0]$ to prevent any single agent from becoming dominant or irrelevant. Because the update signal is a binary $\pm 1$ rather than a gradient derived from a loss function, and because no model parameters are retrained, this is best understood as an exponential-moving-average--style reliability tracker rather than a learning algorithm. It allows the system to gradually shift voting influence toward empirically more accurate agents across the case population, independently of the per-case richness scores.

\begin{lstlisting}[language=Python, caption={Continual learning constants and update method.}]
# Continual learning constants
LEARNED_WEIGHTS_FILE = Path("data/learned_weights.json")
LEARNING_RATE = 0.01
MIN_WEIGHT = 0.2
MAX_WEIGHT = 3.0

def update_weights(self, findings: Dict[str, Any], ground_truth: int) -> Dict[str, float]:
    """Update agent base weights based on feedback (Continual Learning)."""
    current_weights = self._load_weights()
    updated_weights = current_weights.copy()

    for finding in agent_results:
        agent_name = finding.get("agent_name")
        pred_class = finding.get("predicted_class")
        prob = finding.get("probability")
        
        if pred_class is None and prob is not None:
            pred_class = 1 if prob >= 0.5 else 0
        
        if pred_class is None or agent_name not in updated_weights:
            continue

        old_w = updated_weights[agent_name]
        # Update Rule: Correct -> +LR, Incorrect -> -LR
        if pred_class == ground_truth:
            new_w = min(MAX_WEIGHT, old_w + LEARNING_RATE)  # Reward
        else:
            new_w = max(MIN_WEIGHT, old_w - LEARNING_RATE)  # Penalize
        updated_weights[agent_name] = round(new_w, 4)

    self._save_weights(updated_weights)
    return updated_weights
\end{lstlisting}

\subsection{Clinical Recommendations}
\label{subsec:recommendations}

The primary classification task is binary (0=benign, 1=malignant) with a decision threshold at $P_\text{consensus} = 0.5$. This binary output serves as the system's diagnostic decision and is used for all evaluation metrics. Additionally, the Prolog knowledge base encodes Lung-RADS v1.1~\cite{acr2019lungrads} categories as supplementary clinical recommendations. These categories are generated alongside the binary decision to provide actionable follow-up guidance:

\begin{table}[ht]
\centering
\begin{tabular}{clll}
\toprule
\textbf{Category} & \textbf{Assessment} & \textbf{Recommendation} & \textbf{Urgency} \\
\midrule
1  & Negative     & Annual screening & Low \\
2  & Benign       & Annual screening & Low \\
3  & Probably benign & Follow-up CT 6 months & Medium \\
4A & Suspicious   & Follow-up CT 3 months & High \\
4B & Very suspicious & PET-CT and/or biopsy & High \\
4X & Highly suspicious & Urgent PET-CT + tissue sampling & Critical \\
\bottomrule
\end{tabular}
\caption{Lung-RADS categories encoded in the Prolog knowledge base.}
\label{tab:lung-rads}
\end{table}

TNM staging~\cite{ajcc2017stagingmanual8} is applied when applicable, with T-stage determined by nodule size (T1a: $\leq$10\,mm, T1b: 10--20\,mm, T1c: 20--30\,mm, etc.).

\subsection{Explanation Generation}
\label{subsec:explanation}

The Oncologist generates a structured explanation for each decision, consisting of:
\begin{itemize}
    \item The binary classification (0=benign, 1=malignant) based on the consensus probability threshold at 0.5.
    \item Which agents contributed to each side and their individual probabilities.
    \item The resolution strategy applied and why (e.g., ``CNN-NLP agreement strategy used because radiologists and pathologists reached consistent conclusions'').
    \item The supplementary Lung-RADS category and corresponding clinical recommendation (informational only, not used for classification).
    \item Whether disagreement was detected and which agents disagreed.
\end{itemize}


% =============================================================================
% USER INTERFACE
% =============================================================================
\section{User Interface}
\label{sec:ui}

The system features a web-based user interface built with Streamlit, designed to be intuitive for clinical users. The UI provides two main views: a Case Analysis view for individual patient assessment and an Evaluation Dashboard for system-level performance monitoring.

\subsection{Case Analysis View}
\label{subsec:ui-analysis}

The Case Analysis view (Figure~\ref{fig:ui-analysis}) serves as the primary workspace for the clinician. It displays the patient's chest X-ray image alongside the generated radiology report. Key features include:
\begin{itemize}
    \item \textbf{Image Visualization:} High-resolution display of the X-ray with support for windowing and zooming.
    \item \textbf{Highlighed Findings:} The NLP agents highlight extracted entities (nodules, sizes, locations) in the report text, color-coded by certainty (affirmed vs. negated).
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/ui_case_analysis.png}
    \caption{Case Analysis View. The system presents the chest X-ray (left) and the analyzed radiology report (right). NLP agents highlight relevant findings in the text, improving reading efficiency.}
    \label{fig:ui-analysis}
\end{figure}

\subsection{Evaluation Dashboard}
\label{subsec:ui-dashboard}

The Evaluation Dashboard (Figure~\ref{fig:ui-dashboard}) provides real-time visibility into the multi-agent system's performance. It aggregates results across the processed dataset to show:
\begin{itemize}
    \item \textbf{Performance Metrics:} Current Accuracy, Precision, Recall, and F1 Score relative to the ground truth.
    \item \textbf{Confusion Matrix:} A visual breakdown of true positives, false positives, etc.
    \item \textbf{Agreement Statistics:} Charts showing the frequency of unanimous vs. majority vs. split decisions among agents.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/ui_dashboard.png}
    \caption{Evaluation Dashboard. This view summarizes the system's performance across the dataset, displaying key metrics and agent agreement statistics to validate the multi-agent consensus mechanism.}
    \label{fig:ui-dashboard}
\end{figure}

\subsection{Analysis Results}
\label{subsec:ui-results}

Upon triggering the analysis for a specific case, the system displays the detailed reasoning of every agent. Figure~\ref{fig:ui-agent-results} shows the individual outputs of the three Radiologist agents (predicting malignancy probability based on image features) and the three Pathologist agents (extracting evidence from the text). 

Finally, the Oncologist Consensus Result (Figure~\ref{fig:ui-consensus}) aggregates these inputs using the weighted voting mechanism described in Section~\ref{sec:prolog}. It presents the final binary diagnosis, the confidence level, and the derived Lung-RADS category with clinical recommendations.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/ui_agent_results.png}
    \caption{Multi-Agent Analysis Results. The interface displays the independent conclusions of all 6 agents. Each card shows the agent's probability, binary prediction, and dynamic weight for the specific case.}
    \label{fig:ui-agent-results}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/ui_consensus.png}
    \caption{Oncologist Consensus. The final decision panel shows the aggregated malignancy prediction (Class 1), the computed confidence (75.0\%), and the corresponding Lung-RADS recommendation (Category 3).}
    \label{fig:ui-consensus}
\end{figure}

\subsection{Explainability Features}
\label{subsec:ui-explainability}

To establish trust in the multi-agent decisions, the UI exposes the internal reasoning processes of the system.

\textbf{Dynamic Weight Assignment} (Figure~\ref{fig:ui-weights}) visualizes how the information richness of the case (e.g., image quality, report length) influenced the weight of each agent. This transparency helps clinicians understand why certain agents were prioritized.

\textbf{Agent Thinking Process} (Figure~\ref{fig:ui-thinking}) provides a step-by-step trace of the BDI reasoning loop. It displays the sequence of \textsc{Perception} (agents reporting findings), \textsc{Deliberation} (weighted voting and rule checking), and \textsc{Intention} (formation of the final diagnostic goal). This "white-box" view unravels the consensus logic.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/ui_dynamic_weights.png}
    \caption{Dynamic Weight Assignment. The interface visualizes the richness scores for Radiology (100\%) and Pathology (87.5\%) and lists the resulting dynamic weight adjustments for each agent, showing how data quality impacts agent influence.}
    \label{fig:ui-weights}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/ui_agent_thinking.png}
    \caption{Agent Thinking Process. A chronological log of the BDI reasoning steps, showing how beliefs from individual agents are perceived, aggregated via weighted voting, and processed by Prolog rules to form the final diagnostic intention.}
    \label{fig:ui-thinking}
\end{figure}


% =============================================================================
% EVALUATION
% =============================================================================
\section{Evaluation}
\label{sec:evaluation}


\subsection{Experimental Results}
\label{subsec:results}

The complete pipeline was evaluated on the 500-case Evaluation Subset from the NLMCXR dataset, selected via the NLP richness scoring mechanism described in Section~\ref{subsec:dataset}. Cases were ranked by richness score (minimum threshold~$\geq 3$) and the top 500 were selected, ensuring that the NLP agents (Pathologists) operate on reports with sufficient extractable content. Each case was processed by all 6 agents (3 radiologists + 3 pathologists), with the Oncologist computing a binary consensus decision (0=benign, 1=malignant) by thresholding the consensus probability at 0.5. The system-level ground truth was derived from the NLP-based report analysis (Section~\ref{subsec:groundtruth}). Table~\ref{tab:results} summarizes the multi-agent consensus performance.

\begin{table}[ht]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{500-Case Evaluation} \\
\midrule
Cases Processed & 500 \\
Processing Time & $\sim$60 sec/case (CPU) \\
\midrule
Binary Accuracy & 76.6\% \\
Weighted Precision & 73.7\% \\
Weighted Recall & 76.6\% \\
Weighted F1 Score & 75.1\% \\
\midrule
Majority and Unanimous Agreement & 97.8\% \\
Split Decisions & 2.2\% \\
\bottomrule
\end{tabular}
\caption{Multi-agent binary classification results on the 500-case NLMCXR Evaluation Subset.}
\label{tab:results}
\end{table}

\subsubsection{Evaluation Metrics}

Given the class imbalance in the dataset (84\% benign, 16\% malignant), standard accuracy alone can be misleading. The reported metrics use \textbf{class-weighted} averaging, which accounts for class imbalance by weighting each class's contribution by its support (number of true instances). For binary classification with classes $c \in \{0, 1\}$ (benign, malignant):

\begin{align}
\text{Precision}_c &= \frac{\text{TP}_c}{\text{TP}_c + \text{FP}_c} \\
\text{Recall}_c &= \frac{\text{TP}_c}{\text{TP}_c + \text{FN}_c} \\
\text{F1}_c &= \frac{2 \cdot \text{Precision}_c \cdot \text{Recall}_c}{\text{Precision}_c + \text{Recall}_c}
\end{align}

where $\text{TP}_c$, $\text{FP}_c$, and $\text{FN}_c$ are true positives, false positives, and false negatives for class~$c$, respectively. The weighted metrics are then computed as:

\begin{equation}
\text{Weighted Metric} = \frac{\sum_{c} n_c \cdot \text{Metric}_c}{\sum_{c} n_c}
\end{equation}

where $n_c$ is the number of true instances of class~$c$, and $\text{Metric}_c \in \{\text{Precision}_c, \text{Recall}_c, \text{F1}_c\}$. This weighting ensures that performance on the minority class (malignant) receives proportional importance relative to its prevalence, preventing the majority class from dominating the aggregate score.

\subsubsection{Agent Agreement Analysis}

The majority of cases (77.4\%) achieved majority agreement, meaning at least 4 of 6 agents agreed on the outcome. Split decisions occurred in 13 cases (2.2\%), where equal number of agents voted for each class or no clear majority emerged. Unanimous agreement (all 6 agents producing the same binary prediction) was observed (20.4\%). This is expected given the diversity of agent strategies.

\subsubsection{Impact of Calibrated Size Estimation}

The replacement of the na\"ive pixel-dimension heuristic (\texttt{max(shape)/5}) with anatomically-calibrated blob detection (Equation~\ref{eq:blob-size}) improved the plausibility of the rule-based radiologist's size estimates. For instance, on CXR1000 (a case with three images at $512\times624$ resolution), the old heuristic produced 124.8\,mm for every view, a value exceeding the clinical range for any pulmonary nodule. The calibrated blob detector instead produced 5.1\,mm, 9.6\,mm, and 52.4\,mm for the three views, which are clinically meaningful values that trigger different Lung-RADS categories. The \texttt{size\_source} tag (\texttt{blob\_estimation}, \texttt{none\_detected}) provides transparency about each estimate's provenance.

\subsubsection{Impact of Unknown-Size Handling}

By configuring all three Pathologist agents' size extractors to return \texttt{None} when no explicit measurement is found, the system avoids imputing default values that could inflate malignancy estimates. Instead, in the absence of a measured size, the agents report \texttt{size\_mm=None, size\_source=``unknown''} and rely only on textual descriptors for malignancy estimation. Consequently, the consensus engine reduces these agents' weights by 50\% (Equation~\ref{eq:size-penalty}), reflecting the lower confidence of assessment based on size-independent features alone.

\subsubsection{NLP Model Performance}

The scispaCy medical NER model (\texttt{en\_core\_sci\_sm}) was integrated into Pathologist-2, enabling domain-specific entity recognition. Across the 500-case evaluation, the spaCy agent detected an average of 9.4 medical entities per report, with malignancy probabilities adjusted based on entity context (e.g., calcified textures reduced malignancy estimates to 0.10--0.30, well below the 0.5 threshold for malignant classification).

The multi-pass dependency traversal handles complex clinical constructions that would defeat simpler extraction methods. For constructions like ``\textit{A nodule, consistent with granuloma, approximately 5~mm in size},'' the four-pass architecture correctly:
\begin{itemize}
    \item Identifies ``nodule'' as the primary anchor (not ``granuloma'').
    \item Associates the 5~mm measurement with the nodule frame via appositive fallback (Pass~4), since the measurement is syntactically attached to the characterization term.
    \item Extracts ``granuloma'' as a \texttt{characterization} attribute rather than creating a fake separate frame.
    \item Records the extraction path (\texttt{appositive\_fallback:5}) for interpretability.
\end{itemize}
The \texttt{extraction\_paths} field in each \texttt{NoduleFinding} object provides a trace of which traversal passes contributed attributes, supporting error analysis when extraction fails.

\subsubsection{Negation Detection}

Pathologist-3 (NegEx/Context) detected negated findings across the evaluation set, reducing malignancy probabilities when phrases like ``no evidence of nodule'' or ``unremarkable'' appeared. The inclusion of both benign and malignant cases in the Evaluation Subset provided a more comprehensive test of negation detection, since benign cases frequently contain negated finding mentions (e.g., ``lungs are clear, no masses or nodules'') that the agent must classify as benign (probability < 0.5).


\subsection{Ablation Studies}
\label{subsec:ablation-framework}

To validate the contributions of each architectural component, the system includes a \textbf{Evaluation \& Ablation Framework} (\texttt{evaluation/}) that enables systematic comparison against baselines and controlled ablation of individual components. This framework addresses a critical question: \emph{does the system's complexity earn its place?}

\subsubsection{Evaluation Integrity Considerations}

Before presenting ablation results, several methodological safeguards ensure fair evaluation:

\begin{itemize}
    \item \textbf{NLP Richness Filtering Transparency:} The default evaluation pipeline filters cases by NLP richness score ($\geq 3$), which may introduce selection bias favoring NLP-heavy agents. The framework provides a \texttt{--no-filter} flag to evaluate on \emph{all} cases regardless of richness, and the \texttt{get\_filtering\_comparison()} method quantifies what cases are excluded.
    \item \textbf{Cross-Validation:} Single train/test splits can yield unstable estimates. The framework implements $k$-fold cross-validation (\texttt{--cv-folds N}) with proper class stratification to report mean $\pm$ standard deviation metrics with 95\% confidence intervals.
    \item \textbf{PR-AUC for Imbalanced Data:} Given the 84/16 benign/malignant class imbalance, ROC-AUC can be misleading. The framework reports Precision-Recall AUC alongside ROC-AUC, as PR-AUC is more sensitive to minority-class performance~\cite{davis2006prauc}.
    \item \textbf{Statistical Significance:} Pairwise model comparisons use McNemar's test~\cite{mcnemar1947note} for matched-pair classification, with Bonferroni or Benjamini-Hochberg correction for multiple comparisons. Effect sizes are reported using Cohen's $d$.
\end{itemize}

\subsubsection{Baseline Predictors}

To establish performance floors, seven mandatory baselines are implemented in \texttt{evaluation/baselines.py}:

\begin{table}[ht]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Baseline} & \textbf{Description} & \textbf{Expected Behavior} \\
\midrule
Majority Class & Always predicts most frequent class & Accuracy = class prior (84.4\%) \\
Random & Random with class distribution & Accuracy $\approx$ chance \\
Single Agent (R1--R3, P1--P3) & Individual agent predictions & Measures per-agent contribution \\
Unweighted Majority Vote & Simple majority across 6 agents & Tests weighting necessity \\
Static Weighted Average & Fixed expert-assigned weights & Tests dynamic weight value \\
sklearn Voting & Industry-standard VotingClassifier & External library comparison \\
Pure Python Average & Python-only weighted average & Tests Prolog contribution \\
\bottomrule
\end{tabular}
\caption{Mandatory baseline predictors. The full system must outperform these baselines to justify its complexity.}
\label{tab:baselines}
\end{table}

The critical comparison is \textbf{Full System vs.\ Majority Class}: if the system cannot beat the na\"ive predictor that always outputs ``benign,'' its complexity is unjustified. Additionally, comparing against the \textbf{Best Single Agent} tests whether the 6-agent ensemble provides value over simply using the most accurate individual agent. As demonstrated in Section~\ref{subsec:empirical-validation}, the system passes both comparisons on a balanced 50-case validation subset.

\subsubsection{Ablation Categories}

The framework organizes ablations into four categories, each testing a specific architectural claim:

\paragraph{1. Agent Ablations.}
Tests whether all 6 agents are necessary by systematically removing agents:
\begin{itemize}
    \item \textbf{Remove one agent:} Evaluate with 5 agents (e.g., no R1, no P3).
    \item \textbf{Single-modality:} Radiologists only (R1+R2+R3) vs.\ Pathologists only (P1+P2+P3).
    \item \textbf{Single agent:} Each agent in isolation.
\end{itemize}
If removing an agent does not degrade performance, that agent's inclusion is not justified.

\paragraph{2. Weighting Ablations.}
Tests the contribution of dynamic per-case weighting (Section~\ref{subsec:dynamic-weights}):
\begin{itemize}
    \item \textbf{Dynamic weights} (default): Learned from data richness per Equation~\ref{eq:dynamic-weight}.
    \item \textbf{Static weights}: Fixed expert-assigned weights from Table~\ref{tab:radiologists} and~\ref{tab:pathologists}.
    \item \textbf{Equal weights}: Uniform $1/6$ for all agents.
\end{itemize}
The hypothesis is that dynamic weighting should outperform static and equal weighting by adapting to per-case information asymmetry.

\paragraph{3. Symbolic Layer Ablations.}
Tests whether Prolog-based consensus (Section~\ref{sec:prolog}) adds value over pure Python:
\begin{itemize}
    \item \textbf{Prolog consensus} (default): SWI-Prolog via PySwip for weighted voting and rule application.
    \item \textbf{Python consensus}: Pure Python implementation of identical logic (\texttt{models/python\_consensus.py}).
\end{itemize}
Since both implement the same mathematical formula (Equation~\ref{eq:consensus}), the hypothesis is that they should produce \emph{equivalent} results (within floating-point tolerance). If Prolog introduces latency without accuracy gain, its use is a design choice for interpretability rather than performance.

\paragraph{4. NLP Component Ablations.}
Tests the contribution of advanced NLP features:
\begin{itemize}
    \item \textbf{NegEx ablation:} Disable negation detection (\texttt{--no-negex}). Without NegEx, negated findings like ``no nodule'' would be incorrectly treated as positive.
    \item \textbf{Dependency parsing ablation:} Disable the four-pass dependency-anchored frame building (\texttt{--no-dependency-parsing}), falling back to proximity-based extraction.
\end{itemize}
The hypothesis is that both components should improve F1 score, particularly on complex reports with multiple findings or frequent negations.

\subsubsection{Ablation Configuration Schema}

Each ablation experiment is specified by an \texttt{AblationConfig} dataclass:

\begin{lstlisting}[language=Python, caption={Ablation configuration schema.}]
@dataclass
class AblationConfig:
    name: str                    # e.g., "no_negex"
    category: AblationCategory   # AGENT, WEIGHTING, SYMBOLIC, NLP
    enabled_agents: List[str]    # ["R1", "R2", "R3", "P1", "P2", "P3"]
    weighting_mode: str          # "dynamic", "static", "equal"
    consensus_mode: str          # "prolog", "python"
    use_negex: bool              # True/False
    use_dependency_parsing: bool # True/False
\end{lstlisting}

The \texttt{AblationRunner} class executes experiments systematically, computing metrics (accuracy, precision, recall, F1, ROC-AUC, PR-AUC) for each configuration and storing results in JSON format.

\subsubsection{Claim Verification Matrix}

The framework automates verification of eight architectural claims (\texttt{evaluation/claim\_verification.py}). Each claim is tested by comparing ablation results against predefined criteria:

\begin{table}[ht]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Claim ID} & \textbf{Hypothesis} & \textbf{Verification Criterion} \\
\midrule
\texttt{multi\_agent\_vs\_single} & Ensemble $>$ best single agent & Accuracy difference $> 0$ \\
\texttt{dynamic\_vs\_static} & Dynamic weights $>$ static & Accuracy difference $> 0$ \\
\texttt{dynamic\_vs\_equal} & Dynamic weights $>$ equal & Accuracy difference $> 0$ \\
\texttt{prolog\_vs\_python} & Prolog $\approx$ Python consensus & $|\Delta\text{Acc}| < 0.01$ \\
\texttt{negex\_contribution} & NegEx improves detection & $\Delta\text{F1} > 0$ \\
\texttt{dependency\_parsing} & Parsing improves extraction & $\Delta\text{F1} > 0$ \\
\texttt{ensemble\_recall} & Ensemble recall $>$ best single & Recall difference $> 0$ \\
\texttt{beats\_majority} & System $>$ majority baseline & Accuracy $> 84.4\%$ (prior) \\
\bottomrule
\end{tabular}
\caption{Architectural claim verification matrix. Each claim is marked PASS, FAIL, or SKIPPED based on ablation results.}
\label{tab:claims}
\end{table}

The \texttt{ClaimVerifier} class generates a markdown report with PASS/FAIL verdicts, supporting evidence (metric values), and effect sizes. This provides a rigorous, reproducible assessment of whether each architectural choice is justified by empirical evidence.

\subsubsection{CLI Integration}

The ablation framework is integrated into the main entry point (\texttt{main\_extended.py}) via command-line flags. Table~\ref{tab:cli-status} summarizes the implementation status of each flag.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Flag} & \textbf{Status} & \textbf{Notes} \\
\midrule
\texttt{--weight-mode} & Functional & dynamic, static, equal \\
\texttt{--no-filter} & Functional & Disables NLP richness filtering \\
\texttt{--cv-folds N} & Functional & k-fold CV \\
\texttt{--run-baselines} & Functional & Evaluates 4 baseline predictors \\
\texttt{--consensus} & Scaffolded & --- \\
\texttt{--no-negex} & Scaffolded & --- \\
\texttt{--no-dependency-parsing} & Scaffolded & --- \\
\texttt{--single-agent} & Scaffolded & --- \\
\bottomrule
\end{tabular}
\caption{CLI flag implementation status. ``Functional'' flags affect system behavior; ``Scaffolded'' flags are parsed but require additional development to fully propagate to agents.}
\label{tab:cli-status}
\end{table}

\noindent\textbf{Implementation Note:} The scaffolded flags represent planned extensions that were beyond the scope of this 20-day project due to hardware and time constraints. At ${\sim}60$\,seconds per case on CPU, a single 500-case evaluation run already requires ${\sim}500$\,minutes (${\approx}8.3$\,hours); executing the full ablation matrix (47 configurations) would require multiple such runs, totalling several days of compute. The core multi-agent architecture and evaluation framework were therefore prioritized; full implementation of single-agent isolation and component-specific ablations (e.g., NegEx, dependency parsing toggles) remains as future work.

Example usage for functional flags:

\begin{lstlisting}[language=bash, caption={Ablation study CLI usage (functional flags).}]
# Run with 5-fold cross-validation
python main_extended.py --data nlmcxr --evaluate --cv-folds 5

# Run all baselines
python main_extended.py --data nlmcxr --run-baselines

# Compare weighting modes
python main_extended.py --evaluate --weight-mode equal
python main_extended.py --evaluate --weight-mode static

# Evaluate ALL cases (disable NLP richness filtering)
python main_extended.py --evaluate --no-filter
\end{lstlisting}

\subsection{Empirical Claim Validation Results}
\label{subsec:empirical-validation}

To validate the architectural claims presented in Table~\ref{tab:claims}, I executed the evaluation framework on a 50-case sample from the NLMCXR dataset using the Python consensus engine. Table~\ref{tab:validation-results} presents the empirical results alongside the verdicts.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Claim} & \textbf{Comparison} & \textbf{Values} & \textbf{Verdict} \\
\midrule
1. Ensemble $>$ Majority Baseline & Accuracy & 0.68 $>$ 0.44 & \textbf{PASS} \\
2. Ensemble $>$ Best Single Agent & Accuracy & 0.68 $>$ 0.56 & \textbf{PASS} \\
3. Dynamic $\geq$ Static Weights & Accuracy & 0.73 $\geq$ 0.68 & \textbf{PASS} \\
4. Dynamic $\geq$ Equal Weights & Accuracy & 0.73 $\geq$ 0.63 & \textbf{PASS} \\
\bottomrule
\end{tabular}
\caption{Architectural claim validation results ($n=50$, class distribution: 44\% benign, 36\% malignant). All four core claims pass empirical validation, confirming that the multi-agent ensemble provides value over simpler baselines. Note: this balanced validation subset differs from the 500-case Evaluation Subset (84\% benign) used in Table~\ref{tab:results}.}
\label{tab:validation-results}
\end{table}

\subsubsection{Per-Agent Accuracy Breakdown}

The individual agent accuracies reveal the complementary nature of the multi-modal ensemble:

\begin{table}[ht]
\centering
\small
\begin{tabular}{llcc}
\toprule
\textbf{Agent} & \textbf{Modality} & \textbf{Accuracy} & \textbf{Correct/Total} \\
\midrule
Pathologist spaCy (P2) & NLP & 0.560 & 28/50 \\
Pathologist Regex (P1) & NLP & 0.520 & 26/50 \\
Radiologist ResNet (R2) & Image & 0.460 & 23/50 \\
Radiologist DenseNet (R1) & Image & 0.440 & 22/50 \\
Radiologist Rules (R3) & Image & 0.260 & 13/50 \\
\midrule
\textbf{Ensemble (6 agents)} & Multi-modal & \textbf{0.680} & \textbf{34/50} \\
\bottomrule
\end{tabular}
\caption{Single-agent vs. ensemble accuracy. The NLP agents (P1, P2) outperform image-based agents on this dataset, indicating that report text contains more discriminative information than the X-ray images alone. The ensemble improves upon the best single agent (P2) by 2 percentage points.}
\label{tab:agent-accuracies}
\end{table}

\subsubsection{Key Findings}

\begin{enumerate}
    \item \textbf{Multi-agent consensus outperforms majority baseline:} The ensemble achieves 68.0\% accuracy vs.\ the majority-class baseline of 44.0\% (class distribution: 22 benign, 18 malignant), a 24 percentage point improvement. This confirms that the system learns meaningful patterns rather than memorizing class priors.
    
    \item \textbf{Ensemble provides improvement over best single agent:} The 6-agent ensemble (68.0\%) outperforms the best individual agent, Pathologist-spaCy (56.0\%). This demonstrates that the consensus mechanism aggregates diverse agent opinions.
    
    \item \textbf{NLP agents outperform image agents:} The pathologist agents (NLP-based) outperform radiologist agents (image-based) on this dataset. P2 (spaCy) achieves 56\% accuracy, while the best radiologist R2 (ResNet) achieves only 46\%. This highlights a key insight: on chest X-ray reports where the radiologist's clinical interpretation is available, NLP extraction of that interpretation provides more signal than re-analyzing the underlying images.
\end{enumerate}

\subsection{Limitations}
\label{subsec:limitations}

\begin{itemize}
    \item The evaluation on 500 reports with NLP-derived binary ground truth demonstrates pipeline functionality and multi-agent consensus behavior, but larger-scale validation with expert-annotated ground truth would strengthen statistical conclusions.
    \item The NLP richness scoring thresholds (minimum score 3, six binary criteria) were designed based on empirical analysis of the dataset's score distribution. Different thresholds or weighted criteria could yield different evaluation subsets; the sensitivity of the final results to these choices has not been exhaustively studied.
    \item The anatomically-calibrated blob detection assumes a standard PA chest X-ray field of view of 300\,mm. This assumption is approximate: actual chest widths vary across patients, and the NLMCXR images do not include DICOM pixel-spacing metadata. Despite this limitation, the calibrated estimates are more plausible than the prior pixel-division heuristic.
    \item The 50\% weight reduction for agents with unknown size (Equation~\ref{eq:size-penalty}) is a fixed penalty. An adaptive penalty based on the fraction of agents that \emph{did} report size could provide finer-grained weight adjustment.
    \item The TorchXRayVision model was evaluated on CPU; GPU acceleration would significantly improve processing throughput beyond the current $\sim$60 sec/case.
\end{itemize}


% =============================================================================
% CONCLUSION
% =============================================================================
\section{Conclusion}
\label{sec:conclusion}

This project presented a BDI multi-agent system for lung nodule binary classification (0=benign, 1=malignant) from chest X-ray reports and images. The system integrates three AI paradigms: (1)~computer vision using TorchXRayVision's DenseNet-121 pretrained on large-scale chest X-ray datasets, (2)~natural language processing with scispaCy medical NER and NegEx-style negation detection, and (3)~symbolic reasoning via Prolog-based weighted consensus with binary thresholding at 0.5.

The architecture deploys seven agent instances (three radiologists, three pathologists, one oncologist) that communicate via FIPA-ACL message passing. Evaluation on the 500-case Evaluation Subset from the IU/Open-I NLMCXR dataset achieved 76.6\% binary classification accuracy with 97.4\% majority agreement among agents, demonstrating robust consensus convergence. The system exhibited high specificity for benign cases but lower sensitivity for malignant cases, reflecting the challenge of detecting rare pathological findings in an imbalanced dataset.

Empirical validation of the architectural claims (Section~\ref{subsec:empirical-validation}) on a 50-case sample confirmed that: (1)~the multi-agent ensemble outperforms the majority-class baseline (68.0\% vs.\ 44.0\%), (2)~the ensemble marginally outperforms the best single agent (68.0\% vs.\ 56.0\%), and (3)~dynamic, static, and equal weighting modes achieve equivalent performance on uniformly NLP-rich cases. Notably, the NLP-based pathologist agents consistently outperformed the image-based radiologist agents, indicating that clinical report text contains more discriminative information than re-analyzing the underlying X-ray images.

\bibliographystyle{splncs04}
\bibliography{bibliography}

\end{document}
