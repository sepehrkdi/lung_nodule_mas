\documentclass[12pt]{article}
\usepackage[a4paper, total={5.7in, 9in}]{geometry}
\usepackage{url}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning, fit}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10},
  keywordstyle=\color{blue},
  commentstyle=\color{green!50!black},
  stringstyle=\color{red!70!black},
  showstringspaces=false
}

\begin{document}
\title{BDI Multi-Agent System for Lung Nodule Evidence Extraction from Chest X-ray Reports}
\author{Sepehr Khodadadi Hosseinabadi\\
{\em 6660699@studenti.unige.it}}
\maketitle

% =============================================================================
% PROPOSAL PAGE (not counted)
% =============================================================================
\section{Details of the Proposal}
\label{sec:yourProposal}

\subsection{Full Specification of the Proposal}
\label{subsec:specification}

I will build a complete BDI multi-agent system in which all agents are BDI agents that interoperate by message passing, while only the CV Radiologist uses a pre-trained model and the NLP is implemented by me. The use case is lung nodule evidence extraction from chest X-ray radiology reports, paired with the corresponding X-ray images from the IU/Open-I chest X-ray collection (English reports). The dataset contains around 7,470 images and reports. I will use an NLP-based pre-filter to select 30--50 image--report pairs whose reports mention lung nodules, and I will annotate them for evaluation of my NLP outputs (entities + attributes + negation/uncertainty).



\subsection{The Kind of the Proposal}
\label{subsec:kind}

My proposal consists of a creative project.

\subsection{The Range of Points/Difficulty of the Proposal}
\label{subsec:points}

The degree of difficulty for this project is hard.

\pagebreak

% =============================================================================
% INTRODUCTION
% =============================================================================
\section{Introduction}
\label{sec:intro}

Lung cancer is the leading cause of cancer-related death worldwide, and early detection of pulmonary nodules is critical for improving patient outcomes. Radiology reports offer a rich source of structured and unstructured clinical information, yet extracting actionable evidence from free-text\footnote{free-text reports are reports that are not structured in a way that can be easily processed by a computer} reports remains a challenging NLP task. Simultaneously, medical imaging with deep learning has shown promise for automated nodule detection, but individual models vary in their sensitivity and specificity.

This project addresses the problem of \emph{lung nodule evidence extraction and fusion} by combining three complementary AI paradigms within a multi-agent architecture:

\begin{enumerate}
    \item \textbf{Natural Language Processing (NLP):} Extracting structured nodule attributes (size, location, texture, descriptors), detecting mentions, and determining negation/uncertainty status from free-text radiology reports.
    \item \textbf{Computer Vision (CV):} Obtaining nodule suspicion scores from chest X-ray images using the \texttt{TorchXRayVision} library~\cite{cohen2022xrv}, leveraging models pretrained on massive chest X-ray datasets.
    \item \textbf{Symbolic Reasoning:} Combining the outputs of multiple NLP and CV agents using first-order logic rules encoded in Prolog, implementing weighted consensus, conflict detection, and clinical recommendation generation.
\end{enumerate}

The system is organized as a \emph{Belief--Desire--Intention (BDI) multi-agent system}~\cite{rao2005bdi}, where each agent maintains beliefs about the world, desires (goals) it aims to achieve, and intentions (plans) it commits to executing. This architecture is particularly well-suited for medical decision support because it mirrors the clinical workflow: independent specialists (radiologists, pathologists) generate findings that are synthesized by a coordinating physician (oncologist) into a final recommendation.

\subsection{Clinical Motivation}
\label{subsec:clinical-motivation}

In standard clinical practice, the workflow for lung nodule management follows a hub-and-spoke model: radiologists analyze images and produce reports, pathologists analyze tissue or textual evidence, and the treating oncologist synthesizes all findings into a treatment decision~\cite{pillay2016mdt}. Direct communication between radiologists and pathologists is uncommon in routine practice---they work independently and send reports to the treating physician. This clinically accurate architecture is reflected in our system design, where all agents report to the central Oncologist agent rather than communicating peer-to-peer.

\subsection{Dataset and Pre-processing}
\label{subsec:dataset}

The system operates on the IU/Open-I Indiana University Chest X-ray Collection~\cite{demsar2005openi}, which contains approximately 7,470 chest X-ray images paired with free-text radiology reports. This subset size is consistent with prior clinical NLP validation studies, for instance, Chapman et al.\ used 42 reports for NegEx evaluation~\cite{chapman2001negex}. To focus the evaluation on relevant pathology, we implemented an NLP-based pre-filter pipeline:

\begin{enumerate}
    \item \textbf{Ingestion:} The efficient \texttt{NLMCXR\_Loader} scans all 7,470 reports in the dataset.
    \item \textbf{Keyword Filtering:} A custom \texttt{MedicalNLPExtractor} analyzes the \texttt{FINDINGS} and \texttt{IMPRESSION} sections of each report for nodule-specific keywords (``nodule'', ``mass'', ``lesion'', ``opacity'').
    \item \textbf{Negation Check:} A simplified NegEx-style rule filter excludes reports where these keywords appear only in negated contexts (e.g., ``no nodule'', ``without mass'').
    \item \textbf{Subset Selection:} The first 50 cases satisfying these criteria (i.e., having at least one affirmed nodule mention) were selected to form the \emph{Actionable Nodule Subset}.
\end{enumerate}

This filtering process ensures that the multi-agent system is evaluated on cases where clinical intervention is potentially required (Ground Truth = 1/Abnormal).

\subsection{Ground Truth Definition}
\label{subsec:groundtruth}

Two distinct forms of ground truth are used in this project:

\begin{itemize}
    \item \textbf{System-Level Ground Truth (Classification):} For the entire dataset, a binary label (1=Abnormal/Suspicious, 0=Normal/Benign) is automatically derived from the original radiology reports. A case is labeled ``Abnormal'' if the report contains un-negated keywords mapping to Lung-RADS categories 3--4 (``suspicious'', ``malignant'', ``ill-defined''). This label serves as the reference for the Oncologist agent's final consensus decision.
    \item \textbf{NLP-Level Ground Truth (Extraction):} For the 50-case Actionable Nodule Subset, token-level manual annotations were created to evaluate the information extraction performance of the Pathologist agents.
\end{itemize}

\subsection{Contributions}
\label{subsec:contributions}

The main contributions of this project are:

\begin{enumerate}
    \item A complete BDI multi-agent system with 7 agent instances across 3 agent types, communicating via FIPA-compliant message passing.
    \item A custom NLP pipeline for lung nodule evidence extraction implementing: report section splitting with section weighting, entity and attribute extraction, measurement normalization, and NegEx-style negation and uncertainty detection.
    \item A Prolog-based consensus mechanism that performs weighted voting, disagreement detection, conflict resolution, and explanation generation.
    \item A dynamic, per-case weight assignment mechanism that adjusts agent reliability weights based on the information richness of the available radiology images and pathology reports, replacing static hardcoded weights.
    \item An end-to-end evaluation on real clinical reports with manually annotated entity-level ground truth.
\end{enumerate}


% =============================================================================
% BACKGROUND
% =============================================================================
\section{Background and Related Work}
\label{sec:background}

\subsection{NLP in Radiology}
\label{subsec:nlp-radiology}

Pons et al.~\cite{pons2016nlpradiology} present a systematic review of NLP applications in radiology, describing a canonical pipeline consisting of: (i) report section splitting (e.g., separating Findings from Impression); (ii) tokenization and normalization (handling abbreviations and standardizing measurements); (iii) syntactic and semantic processing; and (iv) application-specific extraction. They emphasize that \emph{negation detection} is essential for clinical NLP, since a large proportion of medical concepts in radiology reports appear in negated contexts (e.g., ``no evidence of nodule''). Our NLP pipeline follows this structure, with each stage implemented across three specialized Pathologist agents.

\subsection{Negation and Uncertainty Detection}
\label{subsec:negation}

Chapman et al.~\cite{chapman2001negex} introduced NegEx, a simple algorithm for identifying negated findings in clinical text. The algorithm operates by: (1) detecting \emph{trigger phrases} (e.g., ``no'', ``without'', ``denies'', ``ruled out''); (2) defining a forward or backward \emph{scope window} (typically 5--6 words); and (3) marking any medical entity within that scope as negated. Harkema et al.~\cite{harkema2009context} extended this approach with ConText, adding support for experiencer identification and temporal status.

For uncertainty detection, the CheXpert labeler~\cite{irvin2019chexpert} demonstrated that rule-based approaches can effectively classify radiology report mentions as positive, negative, or uncertain using trigger words such as ``possible'', ``cannot exclude'', ``may represent'', and ``suggestive of''. While CheXpert targets 14 chest X-ray observations, our project applies the same principles to the narrower task of nodule-specific attribute extraction.

\subsection{BDI Multi-Agent Systems}
\label{subsec:bdi}

The Belief--Desire--Intention (BDI) architecture~\cite{rao2005bdi} models rational agents that maintain: \emph{beliefs} (information about the environment), \emph{desires} (goals to achieve), and \emph{intentions} (committed plans). This architecture is implemented using the SPADE-BDI framework~\cite{spadebdi2023}, which uses AgentSpeak(L) plans triggered by belief changes and provides FIPA-compliant inter-agent messaging. In our implementation, `spade\_main.py` serves as the entry point, initializing specialized BDI agents that execute strict AgentSpeak logic defined in `.asl` files.

\subsection{Medical Decision Standards}
\label{subsec:medical-standards}

The Oncologist agent's reasoning layer encodes decision criteria grounded in two publicly documented medical standards:

\begin{itemize}
    \item \textbf{Lung-RADS v1.1}~\cite{acr2019lungrads}: The American College of Radiology's Lung Imaging Reporting and Data System defines categories from 1 (Negative) to 4X (Highly Suspicious) based on nodule size, texture, and morphological features. Although originally defined for low-dose CT screening, it provides a clear, codifiable mapping from nodule evidence to management recommendations.
    \item \textbf{TNM Staging (8th Edition)}~\cite{amin2017ajcc}: The AJCC TNM classification provides tumor staging based on tumor size (T), lymph node involvement (N), and distant metastasis (M). We reference the staging thresholds to ensure the rule base uses medically consistent vocabulary.
\end{itemize}


% =============================================================================
% SYSTEM ARCHITECTURE
% =============================================================================
\section{System Architecture}
\label{sec:architecture}

\subsection{Overview}
\label{subsec:arch-overview}

The system follows a hub-and-spoke architecture with three layers, mirroring the clinical workflow where independent specialists report to a coordinating physician:

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    agent/.style={draw, rounded corners, minimum width=2.2cm, minimum height=1cm, align=center, font=\small},
    rads/.style={agent, fill=blue!15},
    paths/.style={agent, fill=green!15},
    onc/.style={agent, fill=orange!15},
    prolog/.style={agent, fill=yellow!15},
    arr/.style={-{Stealth[length=2.5mm]}, thick},
]
% Radiologists
\node[rads] (r1) at (-3.5, 3) {R1\\Conservative};
\node[rads] (r2) at (0, 3) {R2\\Balanced};
\node[rads] (r3) at (3.5, 3) {R3\\Sensitive};

% Pathologists
\node[paths] (p1) at (-3.5, -3) {P1\\Regex/Rules};
\node[paths] (p2) at (0, -3) {P2\\spaCy/NER};
\node[paths] (p3) at (3.5, -3) {P3\\NegEx/Context};

% Prolog
\node[prolog] (prolog) at (0, 0) {Prolog Consensus\\(Weighted Voting)};

% Oncologist
\node[onc] (onc) at (6, 0) {Oncologist\\(BDI)};

% Arrows
\draw[arr] (r1) -- (prolog);
\draw[arr] (r2) -- (prolog);
\draw[arr] (r3) -- (prolog);
\draw[arr] (p1) -- (prolog);
\draw[arr] (p2) -- (prolog);
\draw[arr] (p3) -- (prolog);
\draw[arr] (prolog) -- (onc);

% Input labels
\node[font=\footnotesize, above=0.1cm of r2] {X-ray Image};
\node[font=\footnotesize, below=0.1cm of p2] {Report Text};

% Output
\node[font=\footnotesize, right=0.1cm of onc] {\begin{tabular}{l}Decision +\\Explanation\end{tabular}};
\end{tikzpicture}
\caption{Multi-agent system architecture. Radiologist agents (R1--R3) process images; Pathologist agents (P1--P3) process report text. Before consensus, the orchestrator computes per-case dynamic weights from information richness (Section~\ref{subsec:dynamic-weights}). All findings and dynamic weights are fused by the Prolog consensus engine, and the Oncologist produces the final decision with explanation.}
\label{fig:architecture}
\end{figure}

\subsection{Agent Descriptions}
\label{subsec:agents}

\subsubsection{Radiologist Agents (R1--R3)}
\label{subsubsec:radiologists}

The system deploys three distinct Radiologist agents to simulate clinical inter-reader variability. Each radiologist agent utilizes the \texttt{TorchXRayVision} library~\cite{cohen2022xrv}, employing a DenseNet-121 architecture~\cite{huang2020densenet} pretrained on a comprehensive aggregation of open chest X-ray datasets (including NIH, CheXpert, MIMIC-CXR, and PadChest). This ensures the features are domain-specific to thoracic pathology. The agents specifically monitor the model's output probability for the ``Nodule'' or ``Mass'' pathology classes. No additional CV training is performed; we rely on these robust frozen weights. The system operates in \emph{strict mode}: the CNN model must be available and functional---no heuristic fallbacks are provided. If the model fails to load, a \texttt{ModelUnavailableError} is raised, ensuring classification integrity. The three agents differ in their operating thresholds:

\begin{table}[h]
\centering
\begin{tabular}{llcl}
\toprule
\textbf{Agent} & \textbf{Style} & \textbf{Base Weight} & \textbf{Behavior} \\
\midrule
R1 & Conservative & 1.0 & High specificity, fewer false positives \\
R2 & Balanced     & 1.0 & Standard operating point \\
R3 & Sensitive    & 0.7 & High recall, fewer missed nodules \\
\bottomrule
\end{tabular}
\caption{Radiologist agent configurations. Base weights are dynamically scaled per-case by the information-richness heuristic (Section~\ref{subsec:dynamic-weights}).}
\label{tab:radiologists}
\end{table}

\subsubsection{Pathologist Agents (P1--P3)}
\label{subsubsec:pathologists}

The Pathologist agents are the main NLP contribution, with three active agents analyzing textual evidence. The 3rd agent (Pathologist-3), previously optional, is now fully enabled to handle negation and uncertainty. The term ``Pathologist'' is used metaphorically to represent agents that analyze textual evidence, analogous to how pathologists extract findings from specimens. Each agent implements a different NLP strategy:

\begin{table}[h]
\centering
\begin{tabular}{llcl}
\toprule
\textbf{Agent} & \textbf{Approach} & \textbf{Base Weight} & \textbf{Focus} \\
\midrule
P1 & Regex/Rules  & 0.8 & Robust patterns, section-based extraction \\
P2 & spaCy/NER    & 0.9 & Generalization, synonym handling \\
P3 & NegEx/Context & 0.9 & Negation and uncertainty detection \\
\bottomrule
\end{tabular}
\caption{Pathologist agent configurations. Base weights are dynamically scaled per-case by the information-richness heuristic (Section~\ref{subsec:dynamic-weights}).}
\label{tab:pathologists}
\end{table}

\subsubsection{Oncologist Agent}
\label{subsubsec:oncologist}

The Oncologist agent integrates all outputs using SWI-Prolog (accessed via PySwip~\cite{pyswip2023}). It implements:
\begin{itemize}
    \item Dynamic weight computation: before each case, the Oncologist invokes a \texttt{DynamicWeightCalculator} (Section~\ref{subsec:dynamic-weights}) to produce per-case weights $\hat{w}_i$ from the base weights $w_i$ and the information richness of the available data.
    \item Weighted voting: $P_\text{final} = \frac{\sum_{i} \hat{w}_i \cdot c_i \cdot p_i}{\sum_{i} \hat{w}_i \cdot c_i}$ where $\hat{w}_i$ is the \emph{dynamic} per-case agent weight, $c_i$ the reported confidence, and $p_i$ the malignancy probability.
    \item Conflict detection: disagreement is flagged when the standard deviation of agent probabilities exceeds 0.15.
    \item Resolution strategies: (1) trust CNN radiologists when NLP agrees; (2) use rule-based agent as tiebreaker; (3) conservative recommendation under strong disagreement.
    \item Explanation generation: a natural-language summary of which experts agreed/disagreed, the dynamic weight rationale, and which Prolog rule fired.
\end{itemize}


% =============================================================================
% NLP PIPELINE
% =============================================================================
\section{NLP Pipeline}
\label{sec:nlp}

The NLP pipeline follows the radiology NLP architecture described by Pons et al.~\cite{pons2016nlpradiology}. The pipeline is distributed across the three Pathologist agents, with each agent implementing complementary extraction strategies.

\subsection{Report Section Splitting}
\label{subsec:section-splitting}

Radiology reports in the Open-I collection follow a semi-structured format with standard section headers. The pipeline identifies and segments reports into sections:

\begin{lstlisting}[language=Python, caption={Section splitting with weighting.}]
SECTION_HEADERS = ["FINDINGS:", "IMPRESSION:", "INDICATION:",
                   "TECHNIQUE:", "COMPARISON:"]
SECTION_WEIGHTS = {"IMPRESSION": 1.5, "FINDINGS": 1.0,
                   "INDICATION": 0.5, "TECHNIQUE": 0.2}
\end{lstlisting}

The IMPRESSION section receives a higher weight (1.5$\times$) because it represents the radiologist's synthesis and clinical judgment, following standard clinical practice where impressions carry more diagnostic weight than descriptive findings.

\subsection{Tokenization and Normalization}
\label{subsec:tokenization}

Medical text requires specialized tokenization to handle:
\begin{itemize}
    \item \textbf{Abbreviations:} Common radiology abbreviations (RUL = right upper lobe, GGO = ground-glass opacity, CT, mm) are preserved during tokenization and expanded where needed via a lookup dictionary.
    \item \textbf{Measurement normalization:} Size mentions in different formats (``8 mm'', ``0.8 cm'', ``8mm'') are normalized to millimeters using regex patterns with unit conversion.
    \item \textbf{Hyphenated terms:} Medical compounds (``well-defined'', ``ground-glass'', ``part-solid'') are handled as single tokens.
\end{itemize}

When spaCy/scispaCy~\cite{neumann2019scispacy} is available (Pathologist-2), the pipeline uses its rule-based tokenizer with custom rules for medical text. Otherwise, regex-based tokenization is used (Pathologist-1).

\subsection{Nodule Mention Detection}
\label{subsec:mention-detection}

Nodule mentions are detected using a lexicon-based approach combined with patterns:

\begin{lstlisting}[language=Python, caption={Nodule mention lexicon.}]
NODULE_LEXICON = ["nodule", "nodular", "nodular opacity",
                  "mass", "lesion", "opacity", "tumor",
                  "pulmonary nodule", "lung nodule",
                  "solitary nodule", "spiculated mass"]
\end{lstlisting}

Pathologist-2 supplements this with scispaCy's biomedical NER model, which can recognize medical entities not present in the lexicon.

\subsection{Attribute Extraction}
\label{subsec:attribute-extraction}

Four categories of attributes are extracted:

\begin{enumerate}
    \item \textbf{Anatomical location:} Regex patterns for lobe references (``right upper lobe'', ``RUL''), positional terms (``subpleural'', ``perihilar''), and laterality.
    \item \textbf{Size mentions:} Multiple patterns capture formats including ``15 mm'', ``1.5 cm'', and dimensional notations (``15 $\times$ 12 mm''), with automatic unit normalization to millimeters.
    \item \textbf{Multiplicity:} Detection of plural nodule mentions (``multiple nodules'', ``bilateral'', ``several opacities'', numeric quantifiers such as ``3 nodules'').
    \item \textbf{Descriptors:} Keywords grouped by clinical category:
    \begin{itemize}
        \item Texture: solid, ground-glass, part-solid, subsolid
        \item Margins: well-defined, spiculated, lobulated, poorly-defined
        \item Calcification: popcorn, laminated, central, eccentric, absent
    \end{itemize}
\end{enumerate}

\subsection{Negation Detection}
\label{subsec:negation-detection}

Pathologist-3 implements NegEx-style negation detection~\cite{chapman2001negex, harkema2009context} with the following components:

\textbf{Trigger phrases} are categorized by scope direction:
\begin{itemize}
    \item \emph{Pre-negation} (forward scope): ``no'', ``no evidence of'', ``without'', ``negative for'', ``denies'', ``absence of'', ``rules out'', ``unremarkable''
    \item \emph{Post-negation} (backward scope): ``is ruled out'', ``unlikely'', ``not seen'', ``not identified'', ``not demonstrated''
\end{itemize}

\textbf{Scope determination:} After detecting a trigger, a window of up to 6 words is scanned in the indicated direction. Any nodule-related entity within this window is marked as negated. The scope is terminated early by \emph{termination terms} (``but'', ``however'', ``although'', ``except'') and sentence-ending punctuation.

\textbf{Algorithm:}
\begin{enumerate}
    \item Identify all trigger phrases and their positions in the text.
    \item For each trigger, determine scope boundaries (direction + window + terminators).
    \item For each detected entity, check whether it falls within any trigger's scope.
    \item If within a negation scope, label as \textsc{negated}; otherwise \textsc{affirmed}.
\end{enumerate}

\subsection{Uncertainty Detection}
\label{subsec:uncertainty-detection}

Uncertainty detection follows the same trigger-scope mechanism as negation, using a separate set of trigger phrases drawn from clinical NLP literature~\cite{irvin2019chexpert, harkema2009context}:

\begin{itemize}
    \item \emph{Pre-uncertainty} (forward scope): ``possible'', ``probable'', ``may represent'', ``cannot exclude'', ``cannot rule out'', ``questionable'', ``suspicious for'', ``suggestive of'', ``consistent with'', ``differential includes''
    \item \emph{Post-uncertainty} (backward scope): ``is suspected'', ``is questionable'', ``cannot be excluded'', ``should be considered''
\end{itemize}

Entities within an uncertainty scope are labeled \textsc{uncertain}. When both a negation trigger and an uncertainty trigger apply to the same entity, negation takes precedence, following the convention in CheXpert~\cite{irvin2019chexpert}.

The final output for each entity is a three-valued certainty label: \textsc{affirmed}, \textsc{negated}, or \textsc{uncertain}. These labels are passed to the Oncologist agent, which uses them during conflict resolution (e.g., a negated nodule mention reduces the overall suspicion score).


% =============================================================================
% PROLOG CONSENSUS
% =============================================================================
\section{Prolog-Based Consensus Mechanism}
\label{sec:prolog}

The Oncologist agent uses SWI-Prolog, accessed from Python via PySwip~\cite{pyswip2023}, to implement the consensus and decision logic. A dedicated \texttt{PrologEngine} class (\texttt{knowledge/prolog\_engine.py}) provides the Python--Prolog interface with the following capabilities:

\begin{itemize}
    \item \texttt{query\_lung\_rads(size, texture)}: Returns Lung-RADS category and management recommendation.
    \item \texttt{compute\_consensus(nodule\_id, findings)}: Computes weighted multi-agent consensus.
    \item \texttt{query\_tnm\_stage(nodule\_id, size)}: Returns TNM staging based on tumor characteristics.
\end{itemize}

The engine operates in \emph{strict mode}: if SWI-Prolog is not installed or PySwip cannot initialize, a \texttt{PrologUnavailableError} is raised---no Python fallback logic is provided. This ensures all clinical decisions are derived from the formally verified Prolog knowledge base. The knowledge base is organized into several modules.

\subsection{Agent Registry and Weighted Voting}
\label{subsec:voting}

Each agent is registered with a type and \emph{base} reliability weight:

\begin{lstlisting}[language=Prolog, caption={Agent base weight definitions (defaults, overridden at runtime).}]
:- dynamic agent_weight/2.
agent_weight(radiologist_densenet, 1.0).
agent_weight(radiologist_resnet, 1.0).
agent_weight(radiologist_rules, 0.7).
agent_weight(pathologist_regex, 0.8).
agent_weight(pathologist_spacy, 0.9).
agent_weight(pathologist_negex, 0.9).
\end{lstlisting}

The \texttt{:- dynamic} directive declares \texttt{agent\_weight/2} as a dynamic predicate, allowing the orchestrator to retract the default facts and assert per-case weights (computed by the \texttt{DynamicWeightCalculator}, see Section~\ref{subsec:dynamic-weights}) into the Prolog knowledge base at runtime.

The consensus probability is computed as:
\begin{equation}
P_\text{consensus} = \frac{\sum_{i=1}^{n} \hat{w}_i \cdot p_i}{\sum_{i=1}^{n} \hat{w}_i}
\label{eq:consensus}
\end{equation}

where $\hat{w}_i$ denotes the dynamic per-case weight for agent~$i$ (see Equation~\ref{eq:dynamic-weight}).
Confidence is derived from inter-agent agreement: $\text{Confidence} = \max(0,\; 1 - 3\sigma)$, where $\sigma$ is the standard deviation of agent probabilities.

\subsection{Disagreement Detection and Resolution}
\label{subsec:disagreement}

Disagreement is detected when $\sigma > 0.15$. Two resolution strategies are applied in order:

\begin{enumerate}
    \item \textbf{CNN--NLP agreement:} If CNN radiologists and NLP pathologists reach similar conclusions ($|P_\text{CNN} - P_\text{NLP}| < 0.2$), their weighted combination is used with 60/40 weighting.
    \item \textbf{Rule-based tiebreaker:} If CNN models disagree among themselves, the rule-based radiologist is used as a tiebreaker.
\end{enumerate}

The Prolog consensus engine implements a multi-stage decision pipeline: (1) weighted aggregation of agent probabilities according to Equation~\ref{eq:consensus}; (2) confidence computation via inter-agent agreement; (3) disagreement detection and resolution using CNN--NLP alignment and rule-based tiebreaking; and (4) Lung-RADS category assignment with clinical recommendations and TNM staging where applicable.


Under strong disagreement (split decisions where no majority exists), the system defaults to a conservative recommendation and flags the case for multidisciplinary review.

\subsection{Dynamic Per-Case Weight Assignment}
\label{subsec:dynamic-weights}

A key limitation of static agent weights is the implicit assumption that every case offers the same quality and quantity of information in both modalities. In practice, some cases are accompanied by multiple high-quality X-ray projections but only a terse report, while others contain richly detailed reports but limited or low-quality imaging. Static weights cannot adapt to this per-case asymmetry.

To address this, we replace the fixed weights with a \emph{dynamic, per-case weight assignment} based on an information-richness heuristic. The mechanism is implemented in a dedicated \texttt{DynamicWeightCalculator} module (\texttt{models/dynamic\_weights.py}), which serves as the \emph{single source of truth} for all agent weights, eliminating the previous triplication of weight definitions across the Python agent classes, the Prolog knowledge base, and the orchestrator display code.

\subsubsection{Radiology Richness Score}

For each case, a radiology richness score $R_\text{rad} \in [0,1]$ is computed as a weighted combination of three sub-components:

\begin{equation}
R_\text{rad} = \alpha_1 \cdot S_\text{count} + \alpha_2 \cdot S_\text{PA} + \alpha_3 \cdot S_\text{quality}
\label{eq:rad-richness}
\end{equation}

where $(\alpha_1, \alpha_2, \alpha_3) = (0.35, 0.35, 0.30)$ and:

\begin{itemize}
    \item $S_\text{count}$: Image count score --- 0 images $\to$ 0, 1 image $\to$ 0.5, 2+ images $\to$ 0.8--1.0 (diminishing returns).
    \item $S_\text{PA}$: PA view presence --- 1.0 if a posterior-anterior or frontal view is available (the most diagnostically informative projection for lung nodules), 0.3 otherwise.
    \item $S_\text{quality}$: Image quality proxy --- estimated from image resolution ($\text{height} \times \text{width}$ normalized by a $512 \times 512$ reference), averaged across available views.
\end{itemize}

\subsubsection{Pathology Richness Score}

A pathology richness score $R_\text{path} \in [0,1]$ is computed from four textual sub-components:

\begin{equation}
R_\text{path} = \beta_1 \cdot S_\text{length} + \beta_2 \cdot S_\text{entities} + \beta_3 \cdot S_\text{sections} + \beta_4 \cdot S_\text{certainty}
\label{eq:path-richness}
\end{equation}

where $(\beta_1, \beta_2, \beta_3, \beta_4) = (0.25, 0.30, 0.20, 0.25)$ and:

\begin{itemize}
    \item $S_\text{length}$: Report text length --- character count of combined FINDINGS + IMPRESSION, mapped to $[0,1]$ with saturation at $\sim$300 characters.
    \item $S_\text{entities}$: NLP entity count --- number of medical entities and measurements extracted by the pathologist agents, normalized to $[0,1]$ with saturation at 5 entities.
    \item $S_\text{sections}$: Section completeness --- fraction of key report sections (FINDINGS, IMPRESSION, INDICATION) that are non-empty; e.g., all three present $\to$ 1.0.
    \item $S_\text{certainty}$: Certainty signal --- proportion of \textsc{affirmed} mentions relative to total (affirmed + negated + uncertain); a case dominated by negated findings receives a lower certainty score, reflecting reduced pathological informativeness.
\end{itemize}

\subsubsection{Weight Scaling Formula}

Given the richness scores, each agent's \emph{base weight} $w_i$ is scaled to its \emph{dynamic weight} $\hat{w}_i$ as follows:

\begin{equation}
\hat{w}_i =
\begin{cases}
w_i \cdot \bigl(\lambda + (1 - \lambda) \cdot R_\text{rad}\bigr) & \text{if agent $i$ is a radiologist,} \\
w_i \cdot \bigl(\lambda + (1 - \lambda) \cdot R_\text{path}\bigr) & \text{if agent $i$ is a pathologist,}
\end{cases}
\label{eq:dynamic-weight}
\end{equation}

where $\lambda = 0.5$ is a \emph{scale floor} parameter ensuring that $\hat{w}_i \in [0.5 \cdot w_i,\; w_i]$. The floor guarantees that no agent is ever silenced entirely: even when a modality's data is minimal, the corresponding agents still contribute half their base influence.

\subsubsection{Integration with Prolog}

Before computing consensus for each case, the orchestrator:
\begin{enumerate}
    \item Invokes \texttt{DynamicWeightCalculator.compute\_weights()} with the case metadata.
    \item Calls \texttt{retractall(agent\_weight(Agent, \_))} followed by \texttt{assertz(agent\_weight(Agent, $\hat{w}_i$))} for each agent, overriding the default Prolog facts with the per-case values.
    \item Proceeds with \texttt{calculate\_consensus/3}, which now retrieves the dynamically asserted weights via \texttt{get\_agent\_weight/2}.
\end{enumerate}

This ensures that both the Python-side weighted aggregation and the Prolog-side symbolic reasoning operate on identical, case-specific weights.

\subsubsection{Illustrative Example}

Table~\ref{tab:dynamic-weights-example} shows the dynamic weight computation for three NLMCXR cases that differ in their information profiles.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{2}{c}{\textbf{CXR1}} & \multicolumn{2}{c}{\textbf{CXR10}} & \multicolumn{2}{c}{\textbf{CXR100}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& $\hat{w}$ & Scale & $\hat{w}$ & Scale & $\hat{w}$ & Scale \\
\midrule
$R_\text{rad}$ & \multicolumn{2}{c}{0.728} & \multicolumn{2}{c}{0.728} & \multicolumn{2}{c}{0.728} \\
$R_\text{path}$ & \multicolumn{2}{c}{0.882} & \multicolumn{2}{c}{0.925} & \multicolumn{2}{c}{0.712} \\
\midrule
R1 (DenseNet) & 0.864 & 0.864 & 0.864 & 0.864 & 0.864 & 0.864 \\
R2 (ResNet)   & 0.864 & 0.864 & 0.864 & 0.864 & 0.864 & 0.864 \\
R3 (Rules)    & 0.605 & 0.864 & 0.605 & 0.864 & 0.605 & 0.864 \\
P1 (Regex)    & 0.753 & 0.941 & 0.770 & 0.963 & 0.685 & 0.856 \\
P2 (spaCy)    & 0.847 & 0.941 & 0.866 & 0.963 & 0.770 & 0.856 \\
P3 (Context)  & 0.847 & 0.941 & 0.866 & 0.963 & 0.770 & 0.856 \\
\bottomrule
\end{tabular}
\caption{Dynamic weight scaling on three NLMCXR cases. $\hat{w}$ is the final dynamic weight; Scale = $\hat{w}/w_i$. Radiology richness is identical (all cases have 2 images including a PA view), whereas pathology richness varies with report detail, producing different pathologist weights per case.}
\label{tab:dynamic-weights-example}
\end{table}

\subsection{Clinical Recommendations}
\label{subsec:recommendations}

The Prolog knowledge base encodes Lung-RADS v1.1~\cite{acr2019lungrads} categories and their associated recommendations:

\begin{table}[h]
\centering
\begin{tabular}{clll}
\toprule
\textbf{Category} & \textbf{Assessment} & \textbf{Recommendation} & \textbf{Urgency} \\
\midrule
1  & Negative     & Annual screening & Low \\
2  & Benign       & Annual screening & Low \\
3  & Probably benign & Follow-up CT 6 months & Medium \\
4A & Suspicious   & Follow-up CT 3 months & High \\
4B & Very suspicious & PET-CT and/or biopsy & High \\
4X & Highly suspicious & Urgent PET-CT + tissue sampling & Critical \\
\bottomrule
\end{tabular}
\caption{Lung-RADS categories encoded in the Prolog knowledge base.}
\label{tab:lung-rads}
\end{table}

TNM staging~\cite{amin2017ajcc} is applied when applicable, with T-stage determined by nodule size (T1a: $\leq$10\,mm, T1b: 10--20\,mm, T1c: 20--30\,mm, etc.).

\subsection{Explanation Generation}
\label{subsec:explanation}

The Oncologist generates a structured explanation for each decision, consisting of:
\begin{itemize}
    \item Which agents classified the case as suspicious vs.\ benign.
    \item The resolution strategy applied and why (e.g., ``CNN-NLP agreement strategy used because radiologists and pathologists reached consistent conclusions'').
    \item The Lung-RADS category and corresponding recommendation.
    \item Whether disagreement was detected and which agents disagreed.
\end{itemize}


% =============================================================================
% EVALUATION
% =============================================================================
\section{Evaluation}
\label{sec:evaluation}

\subsection{Annotation Methodology}
\label{subsec:annotation}

To evaluate the specific contribution of the NLP agents (Pathologist-1, P2, P3), the 50 selected reports were manually annotated. The annotation process focused on information extraction targets rather than clinical re-interpretation. This approach is consistent with prior clinical NLP validation studies where non-clinical annotators successfully annotated text spans using explicit guidelines~\cite{chapman2001negex, uzuner2011i2b2}.

\begin{itemize}
    \item \textbf{Entity Spans:} All text spans corresponding to four key entity types were highlighted:
    \begin{itemize}
        \item \textsc{nodule}: The trigger term itself (e.g., ``nodule'', ``mass'').
        \item \textsc{size}: Measurements (e.g., ``15 mm'', ``1.2 cm'').
        \item \textsc{location}: Anatomical location (e.g., ``RUL'', ``left apex'').
        \item \textsc{texture}: Morphological descriptors (e.g., ``spiculated'', ``calcified'').
    \end{itemize}
    \item \textbf{Attributes \& Certainty:} Each \textsc{nodule} entity was tagged with a certainty modifier:
    \begin{itemize}
        \item \textsc{affirmed}: The finding is present (e.g., ``nodule is seen'').
        \item \textsc{negated}: The finding is absent (e.g., ``no nodule'').
        \item \textsc{uncertain}: The finding is equivocal (e.g., ``possible nodule'').
    \end{itemize}
\end{itemize}

These manual annotations serve as the ``Gold Standard'' for calculating the Precision, Recall, and F1-score of the Pathologist agents' extraction modules.

\subsection{Metrics}
\label{subsec:metrics}

NLP extraction is evaluated using:
\begin{itemize}
    \item \textbf{Entity-level Precision, Recall, and F1} for each entity type extracted by the Pathologist agents.
    \item \textbf{Cohen's Kappa} for negation and uncertainty classification agreement with manual annotations.
\end{itemize}

Multi-agent consensus is evaluated by tracking:
\begin{itemize}
    \item Agreement rates: unanimous, majority, and split decisions.
    \item Confidence calibration: the relationship between reported confidence and actual correctness.
\end{itemize}

\subsection{Experimental Results}
\label{subsec:results}

The complete pipeline was evaluated on 100 cases from the NLMCXR dataset. Table~\ref{tab:results} summarizes the multi-agent consensus performance.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{100-Case Run} & \textbf{20-Case Run} \\
\midrule
Cases Processed & 98 & 20 \\
Processing Rate & 4.9 cases/sec & 3.34 cases/sec \\
\midrule
Class 3 (Probably Benign) & 39.8\% & 45.0\% \\
Class 4 (Suspicious) & 60.2\% & 55.0\% \\
\midrule
Majority Agreement & 85.7\% & 85.0\% \\
Split Decisions & 14.3\% & 15.0\% \\
\midrule
Mean Probability & 0.596 & 0.593 \\
Mean Confidence & 0.89 & 0.88 \\
\bottomrule
\end{tabular}
\caption{Multi-agent consensus results on NLMCXR dataset.}
\label{tab:results}
\end{table}

\subsubsection{Agent Agreement Analysis}

The 85\% majority agreement rate indicates that at least 4 of 5 agents (3 radiologists + 2 active pathologists) reached the same classification in most cases. Split decisions (15\%) occurred when radiologist agents disagreed due to different operating thresholds---specifically, the DenseNet-based radiologist (focusing on ``Nodule'' pathology) sometimes classified cases differently from the Mass-focused variant.

\subsubsection{NLP Model Performance}

The scispaCy medical NER model (\texttt{en\_core\_sci\_sm}) was integrated into Pathologist-2, enabling domain-specific entity recognition. Across the 20-case run, the spaCy agent detected an average of 9.4 medical entities per report, with malignancy scores appropriately adjusted based on entity context (e.g., calcified textures reduced malignancy estimates to 0.10--0.30).

\subsubsection{Negation Detection}

Pathologist-3 (NegEx/Context) successfully detected negated findings in 20\% of cases, appropriately reducing malignancy scores when phrases like ``no evidence of nodule'' or ``unremarkable'' appeared. The agent distinguished between affirmed ($n=16$), negated ($n=4$), and uncertain ($n=3$) contexts across the test set.

\subsection{Limitations}
\label{subsec:limitations}

\begin{itemize}
    \item Annotations were performed by a single non-clinical annotator. While the annotation schema targets linguistic phenomena rather than clinical judgment, some edge cases (e.g., distinguishing ``nodule'' from ``mass'') may benefit from clinical expertise. Future work could validate annotations with a radiologist.
    \item The evaluation on 100 reports demonstrates pipeline functionality and multi-agent consensus behavior, but larger-scale validation would strengthen statistical conclusions.
    \item The dynamic weight sub-component coefficients ($\alpha_i$, $\beta_j$) and the scale floor $\lambda = 0.5$ were set heuristically. An empirical sensitivity analysis or learning these parameters from a validation set could further improve calibration.
    \item Lung-RADS v1.1 was originally defined for LDCT screening, not chest X-rays; it is used here as an example of codifiable clinical rules rather than as a direct clinical application.
    \item The TorchXRayVision model was evaluated on CPU; GPU acceleration would significantly improve processing throughput beyond the current 4.9 cases/sec.
\end{itemize}


% =============================================================================
% CONCLUSION
% =============================================================================
\section{Conclusion}
\label{sec:conclusion}

This project presented a BDI multi-agent system for lung nodule evidence extraction from chest X-ray reports. The system integrates three complementary AI paradigms: (1)~computer vision using TorchXRayVision's DenseNet-121 pretrained on large-scale chest X-ray datasets, (2)~natural language processing with scispaCy medical NER and NegEx-style negation detection, and (3)~symbolic reasoning via Prolog-based weighted consensus.

The architecture deploys seven agent instances (three radiologists, three pathologists, one oncologist) that communicate via FIPA-compliant message passing. Evaluation on 100 cases from the IU/Open-I NLMCXR dataset demonstrated 85\% majority agreement among agents, with the consensus mechanism successfully resolving split decisions through weighted voting and conflict detection strategies.

Key technical contributions include:
\begin{itemize}
    \item Integration of domain-specific pretrained models (TorchXRayVision for CV, scispaCy for NLP) within a multi-agent framework.
    \item Implementation of NegEx-style negation and uncertainty detection for radiology report analysis.
    \item A Prolog knowledge base encoding Lung-RADS clinical guidelines for interpretable decision support.
    \item A dynamic, per-case weight assignment mechanism based on information-richness heuristics. Agent reliability weights are scaled at runtime according to the quantity and quality of available radiology images and pathology reports, with the scaling formula (Equation~\ref{eq:dynamic-weight}) guaranteeing a minimum contribution floor of 50\% of the base weight. This approach replaces static hardcoded weights and is synchronized between Python and Prolog via dynamic predicate assertion.
\end{itemize}

% Future work could extend the system to CT imaging using the original Lung-RADS protocol, incorporate additional NLP agents for temporal reasoning (detecting changes over time), and validate the clinical utility of the explanations generated by the Oncologist agent through user studies with radiologists.


\bibliographystyle{splncs04}
\bibliography{bibliography}

\end{document}
